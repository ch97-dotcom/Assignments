{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Provide an example of the concepts of Prior, Posterior, and Likelihood ?\n",
    "Bayes theorem states the following:\n",
    "\n",
    "Posterior = Prior * Likelihood\n",
    "\n",
    "This can also be stated as P (A | B) = (P (B | A) * P(A)) / P(B) , where P(A|B) is the probability of A given B, also called posterior.\n",
    "\n",
    "Prior: Probability distribution representing knowledge or uncertainty of a data object prior or before observing it\n",
    "\n",
    "Posterior: Conditional probability distribution representing what parameters are likely after observing the data object\n",
    "\n",
    "Likelihood: The probability of falling under a specific category or class. \n",
    "2. What role does Bayes' theorem play in the concept learning principle ?\n",
    "Bayes theorem helps to determine the probability of an event with random knowledge. It is used to calculate the probability of occurring one event while other one already occurred. It is a best method to relate the condition probability and marginal probability. \n",
    "3. Offer an example of how the Nave Bayes classifier is used in real life ?\n",
    "In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. \n",
    "4. Can the Nave Bayes classifier be used on continuous numeric data? If so, how can you go about doing it ?\n",
    "we perform classification using Naive Bayes when the data we have is continuous in nature. If an instance in test data set has a category that was not present during training then it will assign it “Zero” probability and won't be able to make prediction. ... It skews the whole performance of the classification. \n",
    "5. What are Bayesian Belief Networks, and how do they work? What are their applications? Are they capable of resolving a wide range of issues ?\n",
    "Probabilistic models can define relationships between variables and be used to calculate probabilities.\n",
    "\n",
    "For example, fully conditional models may require an enormous amount of data to cover all possible cases, and probabilities may be intractable to calculate in practice. Simplifying assumptions such as the conditional independence of all random variables can be effective, such as in the case of Naive Bayes, although it is a drastically simplifying step.\n",
    "\n",
    "An alternative is to develop a model that preserves known conditional dependence between random variables and conditional independence in all other cases. Bayesian networks are a probabilistic graphical model that explicitly capture the known conditional dependence with directed edges in a graph model. All missing connections define the conditional independencies in the model.\n",
    "\n",
    "As such Bayesian Networks provide a useful tool to visualize the probabilistic model for a domain, review all of the relationships between the random variables, and reason about causal probabilities for scenarios given available evidence.\n",
    "\n",
    "In this post, you will discover a gentle introduction to Bayesian Networks. Bayesian networks are a type of probabilistic graphical model comprised of nodes and directed edges.\n",
    "Bayesian network models capture both conditionally dependent and conditionally independent relationships between random variables.\n",
    "Models can be prepared by experts or learned from data, then used for inference to estimate the probabilities for causal or subsequent events.\n",
    "6. Passengers are checked in an airport screening system to see if there is an intruder. Let I be the random variable that indicates whether someone is an intruder I = 1) or not I = 0), and A be the variable that indicates alarm I = 0). If an intruder is detected with probability P(A = 1|I = 1) = 0.98 and a non-intruder is detected with probability P(A = 1|I = 0) = 0.001, an alarm will be triggered, implying the error factor. The likelihood of an intruder in the passenger population is P(I = 1) = 0.00001. What are the chances that an alarm would be triggered when an individual is actually an intruder ?\n",
    "This can be solved directly with the Bayesian theorem.\n",
    "P(T = 1|A = 1) = P(A = 1|T = 1)P(T = 1)\n",
    "P(A = 1) (1)\n",
    "=\n",
    "P(A = 1|T = 1)P(T = 1)\n",
    "P(A = 1|T = 1)P(T = 1) + P(A = 1|T = 0)P(T = 0) (2)\n",
    "=\n",
    "0.98 × 0.00001\n",
    "0.98 × 0.00001 + 0.001 × (1 − 0.00001) = 0.0097 (3)\n",
    "≈\n",
    "0.00001\n",
    "0.001\n",
    "= 0.01 (4)\n",
    "It is interesting that even though for any passenger it can be decided with high reliability (98% and 99.9%)\n",
    "whether (s)he is a terrorist or not, if somebody gets arrested as a terrorist, (s)he is still most likely not a\n",
    "terrorist (with a probability of 99%).\n",
    " \n",
    "7. An antibiotic resistance test (random variable T) has 1% false positives (i.e., 1% of those who are not immune to an antibiotic display a positive result in the test) and 5% false negatives (i.e., 1% of those who are not resistant to an antibiotic show a positive result in the test) (i.e. 5 percent of those actually resistant to an antibiotic test negative). Assume that 2% of those who were screened were antibiotic-resistant. Calculate the likelihood that a person who tests positive is actually immune (random variable D) ?\n",
    "\n",
    "8. In order to prepare for the test, a student knows that there will be one question in the exam that is either form A, B, or C. The chances of getting an A, B, or C on the exam are 30 percent, 20%, and 50 percent, respectively. During the planning, the student solved 9 of 10 type A problems, 2 of 10 type B problems, and 6 of 10 type C problems.\n",
    "What is the likelihood that the student can solve the exam problem?\n",
    "Given the student's solution, what is the likelihood that the problem was of form A?\n",
    " \n",
    "9. A bank installs a CCTV system to track and photograph incoming customers. Despite the constant influx of customers, we divide the timeline into 5 minute bins. There may be a customer coming into the bank with a 5% chance in each 5-minute time period, or there may be no customer (again, for simplicity, we assume that either there is 1 customer or none, not the case of multiple customers). If there is a client, the CCTV will detect them with a 99 percent probability. If there is no customer, the camera can take a false photograph with a 10% chance of detecting movement from other objects.\n",
    "How many customers come into the bank on a daily basis (10 hours)?\n",
    "On a daily basis, how many fake photographs (photographs taken when there is no customer) and how many missed photographs (photographs taken when there is a customer) are there?\n",
    "Explain likelihood that there is a customer if there is a photograph?\n",
    " \n",
    "10. Create the conditional probability table associated with the node Won Toss in the Bayesian Belief network to represent the conditional independence assumptions of the Nave Bayes classifier for the match winning prediction problem in Section 6.4.4.?\n",
    "Bayesian belief network is key computer technology for dealing with probabilistic events and to solve a problem which has uncertainty. We can define a Bayesian network as:\n",
    "\n",
    "\"A Bayesian network is a probabilistic graphical model which represents a set of variables and their conditional dependencies using a directed acyclic graph.\"\n",
    "\n",
    "It is also called a Bayes network, belief network, decision network, or Bayesian model.\n",
    "\n",
    "Bayesian networks are probabilistic, because these networks are built from a probability distribution, and also use probability theory for prediction and anomaly detection.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the concept of supervised learning? What is the significance of the name ?\n",
    "Ans: Supervised learning, also known as supervised machine learning, is a subcategory of machine learning and artificial intelligence. It is defined by its use of labeled datasets to train algorithms that to classify data or predict outcomes accurately.\n",
    "\n",
    "2. In the hospital sector, offer an example of supervised learning ?\n",
    "Ans: Supervised learning uses a training set to teach models to yield the desired output. This training dataset includes inputs and correct outputs, which allow the model to learn over time. The algorithm measures its accuracy through the loss function, adjusting until the error has been sufficiently minimized.\n",
    "\n",
    "3. Give three supervised learning examples ?\n",
    "Ans: Example of Supervised Learning Algorithms are:\n",
    "\n",
    "Linear Regression.\n",
    "Nearest Neighbor.\n",
    "Gaussian Naive Bayes.\n",
    "Decision Trees.\n",
    "Support Vector Machine (SVM)\n",
    "Random Forest.\n",
    "4. In supervised learning, what are classification and regression ?\n",
    "Ans: Fundamentally, classification is about predicting a label and regression is about predicting a quantity. That classification is the problem of predicting a discrete class label output for an example. That regression is the problem of predicting a continuous quantity output for an example.\n",
    "\n",
    "5. Give some popular classification algorithms as examples ?\n",
    "Ans: Popular algorithms that can be used for multi-class classification include:\n",
    "\n",
    "k-Nearest Neighbors\n",
    "Decision Trees.\n",
    "Naive Bayes\n",
    "6. Briefly describe the SVM model ?\n",
    "Ans: A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they're able to categorize new text.\n",
    "7. In SVM, what is the cost of misclassification ?\n",
    "Ans: In cost-sensitive learning instead of each instance being either correctly or incorrectly classified, each class (or instance) is given a misclassification cost. Thus, instead of trying to optimize the accuracy, the problem is then to minimize the total misclassification cost.\n",
    "\n",
    "8. In the SVM model, define Support Vectors ?\n",
    "Ans: Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n",
    "\n",
    "9. In the SVM model, define the kernel ?\n",
    "Ans: SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "10. What are the factors that influence SVM's effectiveness ?\n",
    "Ans: SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.\n",
    "\n",
    "11. What are the benefits of using the SVM model ?\n",
    "Ans: SVM works relatively well when there is a clear margin of separation between classes. SVM is more effective in high dimensional spaces. SVM is effective in cases where the number of dimensions is greater than the number of samples.SVM is relatively memory efficient.\n",
    "\n",
    "12. What are the drawbacks of using the SVM model ?\n",
    "Ans: SVM algorithm is not suitable for large data sets. SVM does not perform very well when the data set has more noise i.e. target classes are overlapping. In cases where the number of features for each data point exceeds the number of training data samples, the SVM will underperform.\n",
    "\n",
    "13. Notes should be written on\n",
    "The kNN algorithm has a validation flaw.\n",
    "In the kNN algorithm, the k value is chosen.\n",
    "A decision tree with inductive bias\n",
    "Ans: The Short notes on below topics is:\n",
    "\n",
    "The kNN algorithm has a validation flaw. The relatively low accuracy of kNN is caused by several factors. One of them is that every characteristic of the method has the same result on calculating distance. The solution of this problem is to give weight to each data characteristic.\n",
    "In the kNN algorithm, the k value is chosen. The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers.\n",
    "\n",
    "A decision tree with inductive bias Shorter trees are preferred over longer ones. Trees that place high information gain attributes close to the root are preferred over those that do not.\n",
    "14. What are some of the benefits of the kNN algorithm ?\n",
    "Ans: Some Advantages of KNN are:\n",
    "\n",
    "Quick calculation time.\n",
    "Simple algorithm – to interpret.\n",
    "Versatile – useful for regression and classification.\n",
    "High accuracy – you do not need to compare with better-supervised learning models.\n",
    "15. What are some of the kNN algorithm's drawbacks ?\n",
    "Ans: Some Disadvantages of KNN are:\n",
    "\n",
    "Accuracy depends on the quality of the data.\n",
    "With large data, the prediction stage might be slow.\n",
    "Sensitive to the scale of the data and irrelevant features.\n",
    "Require high memory – need to store all of the training data.\n",
    "Given that it stores all of the training, it can be computationally expensive.\n",
    "16. Explain the decision tree algorithm in a few words ?\n",
    "Ans: A decision tree is a graphical representation of all the possible solutions to a decision based on certain conditions. Tree models where the target variable can take a finite set of values are called classification trees and target variable can take continuous values (numbers) are called regression trees.\n",
    "\n",
    "17. What is the difference between a node and a leaf in a decision tree ?\n",
    "Ans: A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).\n",
    "\n",
    "18. What is a decision tree's entropy ?\n",
    "Ans: Entropy helps us to build an appropriate decision tree for selecting the best splitter. Entropy can be defined as a measure of the purity of the sub split. Entropy always lies between 0 to 1. The entropy of any split can be calculate by this formula.\n",
    "\n",
    "19. In a decision tree, define knowledge gain ?\n",
    "Ans: Information gain is the reduction in entropy or surprise by transforming a dataset and is often used in training decision trees.Information gain is calculated by comparing the entropy of the dataset before and after a transformation.\n",
    "    \n",
    "20. Choose three advantages of the decision tree approach and write them down ?\n",
    "Ans: Advantages of Decision Trees :\n",
    "\n",
    "Easy to read and interpret. One of the advantages of decision trees is that their outputs are easy to read and interpret without requiring statistical knowledge.\n",
    "Easy to prepare.\n",
    "Less data cleaning required.\n",
    "21. Make a list of three flaws in the decision tree process ?\n",
    "Ans: Issues in Decision Tree Learning :\n",
    "\n",
    "Overfitting the data.\n",
    "Guarding against bad attribute choices.\n",
    "Handling continuous valued attributes.\n",
    "Handling missing attribute values.\n",
    "Handling attributes with differing costs.\n",
    "22. Briefly describe the random forest model ?\n",
    "Ans: The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.\n",
    "    \n",
    "1. Recognize the differences between supervised, semi-supervised, and unsupervised learning ?\n",
    "Ans: The differences between supervised, semi-supervised, and unsupervised learning are:\n",
    "\n",
    "Supervised learning aims to learn a function that, given a sample of data and desired outputs, approximates a function that maps inputs to outputs.\n",
    "\n",
    "Semi-supervised learning aims to label unlabeled data points using knowledge learned from a small number of labeled data points.\n",
    "\n",
    "Unsupervised learning does not have (or need) any labeled outputs, so its goal is to infer the natural structure present within a set of data points.\n",
    "\n",
    "2. Describe in detail any five examples of classification problems ?\n",
    "Ans: 5 Examples of Classification Problems :\n",
    "\n",
    "Logistic regression: Logistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. ... It is one of the simplest ML algorithms that can be used for various classification problems such as spam detection, Diabetes prediction, cancer detection etc.\n",
    "\n",
    "Decision trees: Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter. The tree can be explained by two entities, namely decision nodes and leaves.\n",
    "\n",
    "Random forest: Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression. It performs better results for classification problems.\n",
    "\n",
    "XGBoost: XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.Why XGBoost must be a part of your machine learning toolkit.\n",
    "\n",
    "Light GBM: Light Gradient Boosted Machine, or LightGBM for short, is an open-source library that provides an efficient and effective implementation of the gradient boosting algorithm.\n",
    "\n",
    "3. Describe each phase of the classification process in detail ?\n",
    "Ans: Process of classification consists of two phases :\n",
    "\n",
    "Construction of the classifier\n",
    "Usage of the classifier.\n",
    "A classifier in machine learning is an algorithm that automatically orders or categorizes data into one or more of a set of “classes.” One of the most common examples is an email classifier that scans emails to filter them by class label: Spam or Not Spam.\n",
    "\n",
    "The main goal of the Classification algorithm is to identify the category of a given dataset, and these algorithms are mainly used to predict the output for the categorical data. Multi-class Classifier: If a classification problem has more than two outcomes, then it is called as Multi-class Classifier.\n",
    "\n",
    "4. Go through the SVM model in depth using various scenarios ?\n",
    "Ans: Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges.\n",
    "\n",
    "Support Vectors are simply the coordinates of individual observation. The SVM classifier is a frontier that best segregates the two classes (hyper-plane/ line).\n",
    "\n",
    "SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.\n",
    "\n",
    "5. What are some of the benefits and drawbacks of SVM ?\n",
    "Ans: The following are some of the benefits and drawbacks of SVM:\n",
    "\n",
    "Benefits:\n",
    "SVM works relatively well when there is a clear margin of separation between classes.\n",
    "SVM is more effective in high dimensional spaces.\n",
    "SVM is effective in cases where the number of dimensions is greater than the number of samples.\n",
    "SVM is relatively memory efficient.\n",
    "Drawbacks:\n",
    "SVM algorithm is not suitable for large data sets.\n",
    "SVM does not perform very well when the data set has more noise i.e. target classes are overlapping.\n",
    "In cases where the number of features for each data point exceeds the number of training data samples,the SVM will underperform.\n",
    "6. Go over the kNN model in depth ?\n",
    "Ans: kNN is the simplest machine learning algorithm to understand and also to explain. It is a versatile algorithm i.e. useful for both classification and regression. It has one big advantage is that kNN ha no pre assumption about the data. Let the data speak for itself.\n",
    "\n",
    "The abbreviation KNN stands for K-Nearest Neighbour. It is a supervised machine learning algorithm. The algorithm can be used to solve both classification and regression problem statements. The number of nearest neighbours to a new unknown variable that has to be predicted or classified is denoted by the symbol 'K'.\n",
    "\n",
    "7. Discuss the kNN algorithm's error rate and validation error ?\n",
    "Ans: Training error here is the error you'll have when you input your training set to your KNN as test set. Since your test sample is in the training dataset, it'll choose itself as the closest and never make mistake. For this reason, the training error will be zero when K = 1, irrespective of the dataset.\n",
    "\n",
    "kNN produces predictions by looking at the k nearest neighbours of a case x to predict its y, so that's fine. In particular, the kNN model basically consists of its training cases - but that's the cross validation procedure doesn't care about at all.\n",
    "\n",
    "8. For kNN, talk about how to measure the difference between the test and training results ?\n",
    "\n",
    "Ans: KNN can be used for classification — the output is a class membership (predicts a class — a discrete value). An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors.\n",
    "\n",
    "KNN classifier does not have any specialized training phase as it uses all the training samples for classification and simply stores the results in memory. KNN is a non-parametric algorithm because it does not assume anything about the training data. This makes it useful for problems having non-linear data.\n",
    "\n",
    "9. Create the kNN algorithm ?\n",
    "Ans: The k-nearest neighbor algorithm is imported from the scikit-learn package.\n",
    "\n",
    "Create feature and target variables.\n",
    "Split data into training and test data.\n",
    "Generate a k-NN model using neighbors value.\n",
    "Train or fit the data into the model.\n",
    "Predict the future.\n",
    "10. What is a decision tree, exactly ? What are the various kinds of nodes? Explain all in depth ?\n",
    "Ans: A decision tree is a tree-like model that acts as a decision support tool, visually displaying decisions and their potential outcomes, consequences, and costs. Drawing a decision tree diagram starts from left to right and consists of burst nodes that split into different paths.\n",
    "\n",
    "There are three different types of nodes: chance nodes, decision nodes, and end nodes. A chance node, represented by a circle, shows the probabilities of certain results. A decision node, represented by a square, shows a decision to be made, and an end node shows the final outcome of a decision path.\n",
    "\n",
    "11. Describe the different ways to scan a decision tree ?\n",
    "Ans: In a decision tree analysis, the decision-maker has usually to proceed through the following six steps:\n",
    "\n",
    "Define the problem in structured terms.\n",
    "Model the decision process.\n",
    "Apply the appropriate probability values and financial data.\n",
    "Solve the decision tree.\n",
    "Perform sensitivity analysis.\n",
    "12. Describe in depth the decision tree algorithm ?\n",
    "Ans: Decision Tree algorithm belongs to the family of supervised learning algorithms. The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).\n",
    "\n",
    "13. In a decision tree, what is inductive bias? What would you do to stop overfitting ?\n",
    "Ans: The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered. The kind of necessary assumptions about the nature of the target function are subsumed in the phrase inductive bias.\n",
    "\n",
    "Overfitting makes the model relevant to its data set only, and irrelevant to any other data sets. Some of the methods used to prevent overfitting include ensembling, data augmentation, data simplification, and cross-validation.\n",
    "\n",
    "14.Explain advantages and disadvantages of using a decision tree ?\n",
    "Ans: Advantages and Disadvantages of Decision Trees in Machine Learning. Decision Tree is used to solve both classification and regression problems. But the main drawback of Decision Tree is that it generally leads to overfitting of the data.\n",
    "\n",
    "15. Describe in depth the problems that are suitable for decision tree learning ?\n",
    "Ans: Appropriate Problems for Decision Tree Learning are: Instances are represented by attribute-value pairs. The target function has discrete output values.\n",
    "\n",
    "Disjunctive descriptions may be required.\n",
    "The training data may contain errors.\n",
    "The training data may contain missing attribute values.\n",
    "16. Describe in depth the random forest model. What distinguishes a random forest ?\n",
    "Ans: The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. The fundamental difference is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.\n",
    "\n",
    "17. In a random forest, talk about OOB error and variable value ?\n",
    "Ans: The out-of-bag (OOB) error is the average error for each calculated using predictions from the trees that do not contain in their respective bootstrap sample. There are two measures of importance given for each variable in the random forest. The first measure is based on how much the accuracy decreases when the variable is excluded.The second measure is based on the decrease of Gini impurity when a variable is chosen to split a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. In a linear equation, what is the difference between a dependent variable and an independent variable ?\n",
    "Ans: Algebraically, a linear equation typically takes the form y = mx + b, where m and b are constants, x is the independent variable, y is the dependent variable. The slope tells us how the dependent variable (y) changes for every one unit increase in the independent (x) variable, on average.\n",
    "\n",
    "The variables in a study of a cause-and-effect relationship are called the independent and dependent variables. The independent variable is the cause. Its value is independent of other variables in your study. The dependent variable is the effect\n",
    "\n",
    "2. What is the concept of simple linear regression? Give a specific example ?\n",
    "Ans: Simple linear regression is a regression model that estimates the relationship between one independent variable and one dependent variable using a straight line. Both variables should be quantitative. Linear regression most often uses mean-square error (MSE) to calculate the error of the model.\n",
    "\n",
    "For example, suppose that height was the only determinant of body weight.In this example, if an individual was 70 inches tall, we would predict his weight to be: Weight = 80 + 2 x (70) = 220 lbs. In this simple linear regression, we are examining the impact of one independent variable on the outcome.\n",
    "\n",
    "3. In a linear regression, define the slope ?\n",
    "Ans: The slope of a regression line (b) represents the rate of change in y as x changes. Because y is dependent on x, the slope describes the predicted values of y given x. The slope must be calculated before the y-intercept when using a linear regression, as the intercept is calculated using the slope.\n",
    "\n",
    "4. Determine the graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2) ?\n",
    "Ans: The graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2) is 0\n",
    "\n",
    "5. In linear regression, what are the conditions for a positive slope ?\n",
    "Ans: If the slope is positive, y increases as x increases, and the function runs \"uphill\" (going left to right). If the slope is zero, y does not change, thus is constant—a horizontal line.\n",
    "\n",
    "6. In linear regression, what are the conditions for a negative slope ?\n",
    "Ans: If the slope is negative, y decreases as x increases and the function runs downhill.\n",
    "\n",
    "7. What is multiple linear regression and how does it work ?\n",
    "Ans: Multiple linear regression refers to a statistical technique that uses two or more independent variables to predict the outcome of a dependent variable. The technique enables analysts to determine the variation of the model and the relative contribution of each independent variable in the total variance.\n",
    "\n",
    "8. In multiple linear regression, define the number of squares due to error ?\n",
    "Ans: The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. It's called the mean squared error as you're finding the average of a set of errors.\n",
    "\n",
    "9. In multiple linear regression, define the number of squares due to regression ?\n",
    "Ans: Sum of squares is a statistical technique used in regression analysis to determine the dispersion of data points. In a regression analysis, the goal is to determine how well a data series can be fitted to a function that might help to explain how the data series was generated.\n",
    "    \n",
    "10. In a regression equation, what is multicollinearity ?\n",
    "Ans: Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model. This means that an independent variable can be predicted from another independent variable in a regression model.\n",
    "\n",
    "11. What is heteroskedasticity, and what does it mean ?\n",
    "Ans: Heteroskedasticity (also spelled heteroscedasticity) refers to the error variance, or dependence of scattering, within a minimum of one independent variable within a particular sample.A common cause of variances outside the minimum requirement is often attributed to issues of data quality.\n",
    "\n",
    "12. Describe the concept of ridge regression ?\n",
    "Ans: Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity.By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. It is hoped that the net effect will be to give estimates that are more reliable.\n",
    "\n",
    "13. Describe the concept of lasso regression ?\n",
    "Ans: In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.\n",
    "\n",
    "14. What is polynomial regression and how does it work ?\n",
    "Ans: Polynomial Regression is a form of Linear regression known as a special case of Multiple linear regression which estimates the relationship as an nth degree polynomial. Polynomial Regression is sensitive to outliers so the presence of one or two outliers can also badly affect the performance.\n",
    "\n",
    "15. Describe the basis function ?\n",
    "Ans: This is a generalization of linear regression that essentially replaces each input with a function of the input. (A linear basis function model that uses the identity function is just linear regression.)\n",
    "\n",
    "16. Describe how logistic regression works ?\n",
    "Ans: Logistic regression uses an equation as the representation, very much like linear regression. Input values (x) are combined linearly using weights or coefficient values (referred to as the Greek capital letter Beta) to predict an output value (y).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Using a graph to illustrate slope and intercept, define basic linear regression ?\n",
    "Ans: The equation y=mx+c represents a straight line graphically, where m is its slope/gradient and c its intercept. In this tutorial, you will learn how to plot y=mx+b in Python with Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.linspace(-5,5,100)\n",
    "y = 2*x+1\n",
    "plt.plot(x, y, '-r', label='y=2x+1')\n",
    "plt.title('Graph of y=2x+1')\n",
    "plt.xlabel('x', color='#1C2833')\n",
    "plt.ylabel('y', color='#1C2833')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. In a graph, explain the terms rise, run, and slope ?\n",
    "Ans: The slope of a line measures the steepness of the line. Most of you are probably familiar with associating slope with \"Rise Over Run\". Rise means how many units you move up or down from point to point. On the graph that would be a change in the y values. Run means how far left or right you move from point to point.\n",
    "\n",
    "3. Use a graph to demonstrate slope, linear positive slope, and linear negative slope, as well as the different conditions that contribute to slope ?\n",
    "Ans: The steepness of a hill is called a slope. The same goes for the steepness of a line. The slope is defined as the ratio of the vertical change between two points, the rise, to the horizontal change between the same two points, the run.\n",
    "\n",
    "Slope=Rise/Run=ChangeinY/changeinX\n",
    "\n",
    "The slope of a line is usually represented by the letter m. (x1, y1) represents the first point whereas (x2, y2) represents the second point.\n",
    "\n",
    "M=Y2−Y1/X2−X1\n",
    "\n",
    "It is important to keep the x-and y-coordinates in the same order in both the numerator and the denominator otherwise you will get the wrong slope. download.png\n",
    "\n",
    "4. Use a graph to demonstrate curve linear negative slope and curve linear positive slope ?\n",
    "Ans: Image result for a graph to demonstrate curve linear negative slope and curve linear positive slope.If the signs are different then the answer is negative! If the slope is negative you can plot your next point by going down and right OR up and left. f the slope is positive you can plot your next point by going up and right OR down and left.\n",
    "\n",
    "5. Use a graph to show the maximum and low points of curves ?\n",
    "Ans: To find the maximum/minimum of a curve you must first differentiate the function and then equate it to zero. This gives you one coordinate. To find the other you must resubstitute the one already found into the original function.\n",
    "\n",
    "6. Use the formulas for a and b to explain ordinary least squares ?\n",
    "Ans: This best line is the Least Squares Regression Line (abbreviated as LSRL). This is true where ˆy is the predicted y-value given x, a is the y intercept, b and is the slope. For every x-value, the Least Squares Regression Line makes a predicted y-value that is close to the observed y-value, but usually slightly off.\n",
    "\n",
    "7. Provide a step-by-step explanation of the OLS algorithm ?\n",
    "Ans: Ordinary Least Square Method :\n",
    "\n",
    "Set a difference between dependent variable and its estimation:\n",
    "Square the difference:\n",
    "Take summation for all data.\n",
    "To get the parameters that make the sum of square difference become minimum, take partial derivative for each parameter and equate it with zero.\n",
    "8. What is the regression's standard error? To represent the same, make a graph ?\n",
    "Ans: The standard error of the regression (S), also known as the standard error of the estimate, represents the average distance that the observed values fall from the regression line. Conveniently, it tells you how wrong the regression model is on average using the units of the response variable.\n",
    "\n",
    "9. Provide an example of multiple linear regression ?\n",
    "Ans: Multiple Linear Regression is one of the important regression algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable. Example: Prediction of CO2 emission based on engine size and number of cylinders in a car.\n",
    "\n",
    "10. Describe the regression analysis assumptions and the BLUE principle ?\n",
    "Ans: There are four assumptions associated with a linear regression model:\n",
    "\n",
    "Linearity: The relationship between X and the mean of Y is linear.\n",
    "Homoscedasticity: The variance of residual is the same for any value of X.\n",
    "Independence: Observations are independent of each other.\n",
    "BLUE: is an acronym for the following: Best Linear Unbiased Estimator. In this context, the definition of “best” refers to the minimum variance or the narrowest sampling distribution.\n",
    "11. Describe two major issues with regression analysis ?\n",
    "Ans: It involves very lengthy and complicated procedure of calculations and analysis. It cannot be used in case of qualitative phenomenon viz. honesty, crime etc.\n",
    "\n",
    "The overall idea of regression is to examine two things:\n",
    "\n",
    "Does a set of predictor variables do a good job in predicting an outcome (dependent) variable?\n",
    "Which variables in particular are significant predictors of the outcome variable, and in what way do they–indicated by the magnitude and sign of the beta\n",
    "12. How can the linear regression model's accuracy be improved ?\n",
    "Ans: 8 Methods to Boost the Accuracy of a Model :\n",
    "\n",
    "Add more data. Having more data is always a good idea.\n",
    "Treat missing and Outlier values.\n",
    "Feature Engineering.\n",
    "Feature Selection.\n",
    "Multiple algorithms.\n",
    "Algorithm Tuning.\n",
    "Ensemble methods.\n",
    "13. Using an example, describe the polynomial regression model in detail ?\n",
    "Ans: In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x.For this reason, polynomial regression is considered to be a special case of multiple linear regression.\n",
    "\n",
    "Polynomial regression is one of the machine learning algorithms used for making predictions. For example, it is widely applied to predict the spread rate of COVID-19 and other infectious diseases.\n",
    "\n",
    "14. Provide a detailed explanation of logistic regression ?\n",
    "Ans: Logistic regression is a statistical analysis method used to predict a data value based on prior observations of a data set.Based on historical data about earlier outcomes involving the same input criteria, it then scores new cases on their probability of falling into a particular outcome category.\n",
    "\n",
    "15. What are the logistic regression assumptions ?\n",
    "Ans: Basic assumptions that must be met for logistic regression include independence of errors, linearity in the logit for continuous variables, absence of multicollinearity, and lack of strongly influential outliers.\n",
    "\n",
    "16. Go through the details of maximum likelihood estimation ?\n",
    "Ans: Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values\", are found such that they maximise the likelihood that the process described by the model produced the data that were\", actually observed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point ?\n",
    "Ans: The main distinction between the two approaches is the use of labeled datasets. To put it simply, supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not. Unsupervised learning models, in contrast, work on their own to discover the inherent structure of unlabeled data.\n",
    "\n",
    "In Supervised learning, you train the machine using data which is well “labeled.” Unsupervised learning is a machine learning technique, where you do not need to supervise the model. For example, Baby can identify other dogs based on past supervised learning.\n",
    "\n",
    "2. Mention a few unsupervised learning applications ?\n",
    "Ans: The main applications of unsupervised learning include clustering, visualization, dimensionality reduction, finding association rules, and anomaly detection.\n",
    "\n",
    "3. What are the three main types of clustering methods? Briefly describe the characteristics of each ?\n",
    "Ans: The various types of clustering are:\n",
    "\n",
    "Connectivity-based Clustering (Hierarchical clustering): Hierarchical Clustering is a method of unsupervised machine learning clustering where it begins with a pre-defined top to bottom hierarchy of clusters. It then proceeds to perform a decomposition of the data objects based on this hierarchy, hence obtaining the clusters. This method follows two approaches based on the direction of progress, i.e., whether it is the top-down or bottom-up flow of creating clusters.\n",
    "Centroids-based Clustering (Partitioning methods): Centroid based clustering is considered as one of the most simplest clustering algorithms, yet the most effective way of creating clusters and assigning data points to it. The intuition behind centroid based clustering is that a cluster is characterized and represented by a central vector and data points that are in close proximity to these vectors are assigned to the respective clusters.\n",
    "Density-based Clustering (Model-based methods): If one looks into the previous two methods that we discussed, one would observe that both hierarchical and centroid based algorithms are dependent on a distance (similarity/proximity) metric. The very definition of a cluster is based on this metric. Density-based clustering methods take density into consideration instead of distances. Clusters are considered as the densest region in a data space, which is separated by regions of lower object density and it is defined as a maximal-set of connected points.\n",
    "Distribution-based Clustering: Until now, the clustering techniques as we know are based around either proximity (similarity/distance) or composition (density). There is a family of clustering algorithms that take a totally different metric into consideration – probability. Distribution-based clustering creates and groups data points based on their likely hood of belonging to the same probability distribution (Gaussian, Binomial etc.) in the data.\n",
    "Fuzzy Clustering: The general idea about clustering revolves around assigning data points to mutually exclusive clusters, meaning, a data point always resides uniquely inside a cluster and it cannot belong to more than one cluster. Fuzzy clustering methods change this paradigm by assigning a data-point to multiple clusters with a quantified degree of belongingness metric. The data-points that are in proximity to the center of a cluster, may also belong in the cluster that is at a higher degree than points in the edge of a cluster. The possibility of which an element belongs to a given cluster is measured by membership coefficient that vary from 0 to 1.\n",
    "4. Explain how the k-means algorithm determines the consistency of clustering ?\n",
    "Ans: Calculate the Within-Cluster-Sum of Squared Errors (WSS) for different values of k, and choose the k for which WSS becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow. Within-Cluster-Sum of Squared Errors sounds a bit complex.\n",
    "\n",
    "5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms ?\n",
    "Ans: K-means attempts to minimize the total squared error, while k-medoids minimizes the sum of dissimilarities between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k -means algorithm, k -medoids chooses datapoints as centers ( medoids or exemplars).\n",
    "\n",
    "6. What is a dendrogram, and how does it work? Explain how to do it ?\n",
    "Ans: A dendrogram is a diagram that shows the attribute distances between each pair of sequentially merged classes After each merging, the distances between all pairs of classes are updated. The distances at which the signatures of classes are merged are used to construct a dendrogram.\n",
    "\n",
    "7. What exactly is SSE? What role does it play in the k-means algorithm ?\n",
    "Ans: I am going to refer to it as SSE, which stands for Sum of Squared Errors. The regression line is the line made using the function we defined above. To get the SSE we calculate the distance for each of the data points from the regression line then square the it, then we add to the sum.\n",
    "\n",
    "The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is a measure of error, the objective of k-means is to try to minimize this value. The purpose of this figure is to show that the initialization of the centroids is an important step.\n",
    "\n",
    "8. With a step-by-step algorithm, explain the k-means procedure ?\n",
    "Ans: k-means is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed apriori. The main idea is to define k centers, one for each cluster.\n",
    "\n",
    "Step 1: Choose the number of clusters k.\n",
    "Step 2: Select k random points from the data as centroids.\n",
    "Step 3: Assign all the points to the closest cluster centroid.\n",
    "Step 4: Recompute the centroids of newly formed clusters.\n",
    "Step 5: Repeat steps 3 and 4.\n",
    "9. In the sense of hierarchical clustering, define the terms single link and complete link ?\n",
    "Ans: In single-link (or single linkage) hierarchical clustering, we merge in each step the two clusters whose two closest members have the smallest distance (or: the two clusters with the smallest minimum pairwise distance). Complete-link clustering can also be described using the concept of clique.\n",
    "\n",
    "10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point ?\n",
    "Ans: Apriori algorithm assumes that any subset of a frequent itemset must be frequent. Its the algorithm behind Market Basket Analysis. So, according to the principle of Apriori, if {Grapes, Apple, Mango} is frequent, then {Grapes,Mango} must also be frequent. Here is a dataset consisting of six transactions.\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30. ?\n",
    "Using the k-means method, create two clusters for each set of centroid described above.\n",
    "For each set of centroid values, calculate the SSE.\n",
    " \n",
    "2. Describe how the Market Basket Research makes use of association analysis concepts ?\n",
    "Market Basket Analysis is one of the key techniques used by large retailers to uncover associations between items. It works by looking for combinations of items that occur together frequently in transactions. To put it another way, it allows retailers to identify relationships between the items that people buy. \n",
    "\n",
    "3. Give an example of the Apriori algorithm for learning association rules ?\n",
    " \n",
    "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration ?\n",
    " \n",
    "5. In the k-means algorithm, how do you recompute the cluster centroids ?\n",
    " \n",
    "6. At the start of the clustering exercise, discuss one method for determining the required number of clusters ?\n",
    " \n",
    "7. Discuss the k-means algorithm's advantages and disadvantages ?\n",
    " \n",
    "8. Draw a diagram to demonstrate the principle of clustering ?\n",
    "\n",
    "9. During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. The clusters C1, C2, and C3 have the following findings after the first iteration ?\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "C3: (5,5) and (9,9)\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this clustering's SSE be?\n",
    "\n",
    " \n",
    "10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. What is the underlying concept of Support Vector Machines ?\n",
    " \n",
    "2. What is the concept of a support vector ?\n",
    " \n",
    "3. When using SVMs, why is it necessary to scale the inputs ?\n",
    " \n",
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance ?\n",
    " \n",
    "5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem ?\n",
    " \n",
    "6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C ?\n",
    " \n",
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set ?\n",
    " \n",
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours ?\n",
    "\n",
    "9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve ?\n",
    " \n",
    "10. On the California housing dataset, train an SVM regressor ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Assignment-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set ?\n",
    " \n",
    "2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater ?\n",
    " \n",
    "3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set ?\n",
    " \n",
    "4. Explain if its a good idea to try scaling the input features if a Decision Tree underfits the training set ?\n",
    " \n",
    "5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances ?\n",
    " \n",
    "6. Will setting presort=True speed up training if your training set has 100,000 instances ?\n",
    " \n",
    "7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:\n",
    "To build a moons dataset, use make moons(n samples=10000, noise=0.4).\n",
    "Divide the dataset into a training and a test collection with train test split().\n",
    "To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes.\n",
    "Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent.\n",
    "8. Follow these steps to grow a forest:\n",
    "Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class.\n",
    "Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is the reason ?\n",
    "Ans: Hybrid Model: A technique that combines two or more different machine learning models in some way. But we can't get 95 percent precision as all other models give different precision rate accuracy is differed.\n",
    "\n",
    "2. What's the difference between hard voting classifiers and soft voting classifiers ?\n",
    "Ans: In hard voting (also known as majority voting), every individual classifier votes for a class, and the majority wins. In statistical terms, the predicted target label of the ensemble is the mode of the distribution of individually predicted labels.\n",
    "\n",
    "In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier's importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote.\n",
    "\n",
    "3. Is it possible to distribute a bagging ensemble's training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options ?\n",
    "Ans: When sampling is performed without replacement, it is called pasting. In other words, both approaches are similar.In both cases you are sampling the training data to build multiple instances of a classifier.\n",
    "\n",
    "Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. It is the best starting point for understanding boosting.\n",
    "\n",
    "4. What is the advantage of evaluating out of the bag ?\n",
    "Ans: The advantage of the OOB method is that it requires less computation and allows one to test the model as it is being trained.\n",
    "\n",
    "5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests ?\n",
    "Ans: Random forest uses bootstrap replicas, that is to say, it subsamples the input data with replacement, whereas Extra Trees use the whole original sample. This may increase variance because bootstrapping makes it more diversified.\n",
    "\n",
    "Random forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model. Extra Trees is much faster.\n",
    "\n",
    "This is because instead of looking for the optimal split at each node it does it randomly.\n",
    "\n",
    "6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data ?\n",
    "Ans: If your AdaBoost ensemble underfits the training data, you can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. You may also try slightly increasing the learning rate.\n",
    "    \n",
    "7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set ?\n",
    "Ans: If your Gradient Boosting ensemble overfits the training set, you should try decreasing the learning rate. You could also use early stopping to find the right number of predictors (you probably have too many)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
