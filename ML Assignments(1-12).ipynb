{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What does one mean by the term \"machine learning\" ?\n",
    "Ans: Machine learning Popularly known as ML is a branch of Artificial Intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.\n",
    "\n",
    "2.Can you think of 4 distinct types of issues where it shines ?\n",
    "Ans: The following are some of the issues where Machine Learning can be used:\n",
    "\n",
    "Image Recognition: Image recognition is one of the most common applications of machine learning. It is used to identify objects, persons, places, digital images, etc. The popular use case of image recognition and face detection is Automatic friend tagging suggestion.\n",
    "Speech Recognition: While using Google, we get an option of Search by voice, it comes under speech recognition, and it's a popular application of Machine Learning. Speech recognition is a process of converting voice instructions into text, and it is also known as Speech to text, or Computer based speech recognition At present, machine learning algorithms are widely used by various applications of speech recognition. Google assistant, Siri, Cortana, and Alexa are using speech recognition technology to follow the voice instructions.\n",
    "Traffic prediction: It predicts the traffic conditions such as whether traffic is cleared, slow-moving, or heavily congested with the help of two ways: Real Timelocation of the vehicle form Google Map app and sensors Average time has taken on past days at the same time.\n",
    "Product recommendations: Machine learning is widely used by various e-commerce and entertainment companies such as Amazon, Netflix, etc., for product recommendation to the user.\n",
    "3.What is a labeled training set, and how does it work ?\n",
    "Ans: You split up the data containing known response variable values into two pieces. The training set is used to train the algorithm, and then you use the trained model on the test set to predict the response variable values that are already known.\n",
    "\n",
    "4.What are the two most important tasks that are supervised ?\n",
    "Ans: The two most common supervised learning tasks are Regression and Classification.\n",
    "\n",
    "5.Can you think of four examples of unsupervised tasks ?\n",
    "Ans: Four common Unsupervised Tasks included Clustering, Visualization, Dimensionality Reduction, and Association Rule Learning.\n",
    "\n",
    "6.State the machine learning model that would be best to make a robot walk through various unfamiliar terrains ?\n",
    "Ans: The best Machine Learning algorithm to allow a Robot to walk in unfamiliar terrains is Reinforced Learning, where the robot can learn from response of the terrain to optimize itself.\n",
    "\n",
    "7.Which algorithm will you use to divide your customers into different groups ?\n",
    "Ans: The Best Algorithm to Segment Customers into different groups is either Supervised Learning (if the groups have known labels) or Unsupervised Learning (if there are no group labels).\n",
    "\n",
    "8.Will you consider the problem of spam detection to be a supervised or unsupervised learning problem ?\n",
    "Ans: Spam detection is a Supervised Machine Learning problem because the labels are known (spam or no spam).\n",
    "\n",
    "9.What is the concept of an online learning system ?\n",
    "Ans: Online learning system is a learning system in which the machine learns continously, as data is given in small streams continuously.\n",
    "\n",
    "10.What is out-of-core learning, and how does it differ from core learning ?\n",
    "Ans: Out-of-core learning system is a system that can handle data that cannot fit into your computer memory. It uses online learning system to feed data in small bits.\n",
    "\n",
    "11.What kind of learning algorithm makes predictions using a similarity measure ?\n",
    "Ans: Learning algorithm that relies on a similarity measure to make predictions is Instance Based Algorithm.\n",
    "\n",
    "12.What's the difference between a model parameter and a hyperparameter in a learning algorithm ?\n",
    "Ans: Model parameter determines how a model will predict given a new instance. Model usually has more than one parameter (i.e. slope of a linear model). Hyperparameter is a parameter for the learning algorithm, not of a model.\n",
    "\n",
    "13.What are the criteria that model-based learning algorithms look for? What is the most popular method they use to achieve success? What method do they use to make predictions ?\n",
    "Ans: Model based learning algorithm search for the optimal value of parameters in a model that will give the best results for the new instances. We often use a cost function or similar to determine what the parameter value has to be in order to minimize the function. The model makes prediction by using the value of the new instance and the parameters in its function.\n",
    "\n",
    "14.Can you name four of the most important Machine Learning challenges ?\n",
    "Ans: Four main challenges in Machine Learning include the following:\n",
    "\n",
    "Overfitting the Data (using a model too complicated)\n",
    "Underfitting the data (using a simple model)\n",
    "Lacking in Data\n",
    "Non Representative Data.\n",
    "15.What happens if the model performs well on the training data but fails to generalize the results to new situations? Can you think of three different options ?\n",
    "Ans: If the model performs poorly to new instances, then it has overfitted on the training data. To solve this, we can do any of the following three:\n",
    "\n",
    "Get more data\n",
    "Implement a simpler model\n",
    "Eliminate outliers or noise from the existing data set.\n",
    "16.What exactly is a test set, and why would you need one ?\n",
    "Ans: Test set is a set to test your model (fit using training data) to see how it performs.Test set is necessary to determine how good (or bad) a model performs.\n",
    "\n",
    "17.What is a validation set's purpose ?\n",
    "Ans: Validation set is a set used to compare between different training models.\n",
    "\n",
    "18.What precisely is the train-dev kit, when will you need it, how do you put it to use ?\n",
    "Ans: Cross-validation is a tool to compare models without needing a separate validation set. It is preferred over validation set because we can save from breaking of part of the training set to create a validation set, as having more data is valuable regardless.\n",
    "\n",
    "19.What could go wrong if you use the test set to tune hyperparameters ?\n",
    "Ans: If you tune hyperparameters using the test sets, then it may not perform well on the out-of-sample data because the model is tuned just for that specific set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the concept of human learning? Please give two examples ?\n",
    "Ans: Human learning is the form of learning which requires higher order mental processes like thinking, reasoning, intelligence, etc.We learn different concepts from childhood. For example:\n",
    "\n",
    "Learning through Association - Classical Conditioning.\n",
    "Learning through consequences – Operant Conditioning.\n",
    "Learning through observation – Modeling/Observational Learning.\n",
    "2. What different forms of human learning are there? Are there any machine learning equivalents ?\n",
    "Ans: Different Forms of ML are as follows :\n",
    "\n",
    "Artificial Intelligence Learning Theories. Machine Learning. Reinforcement Learning. Supervised Learning. Unsupervised Learning.\n",
    "ML equivalents like Linear regression, decision trees, random forest and support vector machines are some commonly used techniques that are actually examples of supervised learning.\n",
    "3. What is machine learning, and how does it work? What are the key responsibilities of machine learning ?\n",
    "Ans: Machine learning is a branch of Artificial intelligence (AI) that teaches computers on how to think in a similar way to how humans do, like by Learning and improving upon past experiences.\n",
    "\n",
    "It works by exploring data and identifying patterns, and involves minimal human intervention.\n",
    "Roles and responsibilities of a machine learning engineer are:\n",
    "\n",
    "Designing ML systems.\n",
    "Researching and implementing ML algorithms and tools. Selecting appropriate data sets.\n",
    "Picking appropriate data representation methods. Identifying differences in data distribution that affects model performance. Verifying data quality.\n",
    "4. Define the terms \"penalty\" and \"reward\" in the context of reinforcement learning ?\n",
    "Ans: A Reinforcement Learning Algorithm, which may also be referred to as an agent, learns by interacting with its environment. The agent receives rewards by performing correctly and penalties for performing incorrectly. The agent learns without intervention from a human by maximizing its reward and minimizing its penalty.\n",
    "\n",
    "5. Explain the term \"learning as a search\" ?\n",
    "Ans: Learning can be viewed as a search through the space of all sentences in a concept description language for a sentence that best describes the data. Alternatively, it can be viewed as a search through all hypotheses in a hypothesis space.\n",
    "\n",
    "6. What are the various goals of machine learning? What is the relationship between these and human learning ?\n",
    "Ans: The Goal of machine learning, closely coupled with the goal of AI, is to achieve a through understanding about the nature of learning process (both human learning and other forms of learning), about the computational aspects of learning behaviors, and to implant the learning capability in computer systems.\n",
    "    \n",
    "Humans have the ability to learn, however with the progress in artificial intelligence, machine learning has become a resource which can augment or even replace human learning. Learning does not happen all at once, but it builds upon and is shaped by previous knowledge.\n",
    "\n",
    "7. Illustrate the various elements of machine learning using a real-life illustration ?\n",
    "Ans: The Various elements of the the Machine Learning are:\n",
    "\n",
    "Data\n",
    "Task\n",
    "Model\n",
    "Loss Function\n",
    "Learning Algorithm\n",
    "Evaluation\n",
    "8. Provide an example of the abstraction method ?\n",
    "Ans: In Machine Learning, Abstraction is supported primarily at the level of modules. This can be justified in two ways: first, Data abstraction is mostly a question of program interfaces and therefore it arises naturally at the point where we have to consider program composition and modules.\n",
    "\n",
    "9. What is the concept of generalization? What function does it play in the machine learning process ?\n",
    "Ans: Generalization refers to your model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.\n",
    "\n",
    "In machine learning, generalization is a definition to demonstrate how well is a trained model to classify or forecast unseen data.This issue can result to classify an actual dog image as a cat from the unseen dataset. Therefore, data diversity is very important factor in order to make a good prediction.\n",
    "\n",
    "10. What is classification, exactly? What are the main distinctions between classification and regression ?\n",
    "Ans: In Machine Learning, Classification refers to a predictive modeling problem where a class label is predicted for a given example of input data.Classification is the task of predicting a discrete class label. Whereas Regression is the task of predicting a continuous quantity.\n",
    "\n",
    "11. What is regression, and how does it work? Give an example of a real-world problem that was solved using regression ?\n",
    "Ans: Regression is a Supervised Machine Learning technique which is used to predict continuous values. The ultimate goal of a regression algorithm is to plot a best-fit line or a curve between the data.\n",
    "\n",
    "The three main metrics that are used for evaluating the trained regression model are Variance, Bias and Error.\n",
    "\n",
    "A simple linear regression real life example could mean you finding a relationship between the revenue and temperature, with a sample size for revenue as the dependent variable. In case of multiple variable regression, you can find the relationship between temperature, pricing and number of workers to the revenue.\n",
    "\n",
    "12. Describe the clustering mechanism in detail ?\n",
    "Ans: Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.\n",
    "\n",
    "13. Make brief observations on two of the following topics ?\n",
    "Machine learning algorithms are used\n",
    "Studying under supervision\n",
    "Studying without supervision\n",
    "Reinforcement learning is a form of learning based on positive reinforcement.\n",
    "Ans: The breif observations on the following two topics is:\n",
    "\n",
    "Machine learning algorithms are used: At its Most basic, Machine Learning uses programmed algorithms that receive and analyse input data to predict output values within an acceptable range. As new data is fed to these algorithms, they learn and optimise their operations to improve performance, developing intelligence over time.\n",
    "\n",
    "Studying Under Supervision: In machine learning, there are two important categories- Supervised and Unsupervised learning.Supervised learning, an algorithm learns from a training dataset. We know the correct answers or desired output, the algorithm makes predictions using the given dataset and is corrected by the “supervisor”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Explain the term machine learning, and how does it work? Explain two machine learning applications in the business world. What are some of the ethical concerns that machine learning applications could raise ?\n",
    "Ans: Machine Learning is a form of artificial intelligence (AI) that teaches computers to think in a similar way to how humans do: Learning and improving upon past experiences. It works by exploring data and identifying patterns, and involves minimal human intervention.\n",
    "\n",
    "There are various applications in Business World :\n",
    "\n",
    "Real-time chatbot agents.\n",
    "Decision support.\n",
    "Customer recommendation engines.\n",
    "Customer churn modeling.\n",
    "Dynamic pricing tactics.\n",
    "Market research and customer segmentation.\n",
    "Fraud detection.\n",
    "Also there are some of the ethical concerns that ML applications could raise : AI presents three major areas of ethical concern for society: Privacy and surveillance, bias and discrimination, and perhaps the deepest, most difficult philosophical question of the era, the role of human judgment.\n",
    "\n",
    "2. Describe the process of human learning ?\n",
    "Under the supervision of experts\n",
    "With the assistance of experts in an indirect manner\n",
    "Self-education\n",
    "Ans: The processes of Human learning are described below:\n",
    "\n",
    "Under the supervision of experts: Human-guided machine learning is a process whereby subject matter experts accelerate the learning process by teaching the technology in real-time. For example, if the machine learning model comes across a piece of data it is uncertain about, a human can be asked to weigh in and give feedback. The model then learns from this input, and uses it to make a more accurate prediction the next time. Human-guided machine learning works from the bottom up by first using algorithms to conduct the heavy lifting of identifying relationships within the data, and engaging humans when necessary for training or validation Concept Learning.\n",
    "\n",
    "With the assistance of experts in an indirect manner: Well The process of an algorithm learning from the training dataset can be thought of as a teacher supervising the learning process. We know the correct answers, the algorithm iteratively makes predictions on the training data and is corrected by the teacher. Learning stops when the algorithm achieves an acceptable level of performance Operant Conditioning.\n",
    "\n",
    "Self education: Ability to recognize patterns, learn from data, and become more intelligent over time (can be AI or programmatically based).Machine Learning: AI systems with ability to automatically learn and improve from experience without being explicitly programmed via training Hebbian Learning.\n",
    "\n",
    "3. Provide a few examples of various types of machine learning ?\n",
    "Ans: Examples of Various types of Machine Learning techniques are:\n",
    "\n",
    "Example of Supervised Learning is text classification problems. In this set of problems, the goal is to predict the class label of a given piece of text. One particularly popular topic in text classification is to predict the sentiment of a piece of text, like a tweet or a product review,Image segmentation, Medical Diagnosis\n",
    "\n",
    "Example of Unsupervised Learning : Fraud detection, Malware detection, Anomaly detection, Clustering Analysis, Identification of human errors during data entry Conducting accurate basket analysis, etc.\n",
    "\n",
    "Example of Reinforcement Learning : Applications in self-driving cars, Industry automation : learning-based robots are used to perform various tasks.\n",
    "\n",
    "4. Examine the various forms of machine learning ?\n",
    "Ans: These are three types of Machine Learning Techniques:\n",
    "\n",
    "Supervised Learning\n",
    "Unsupervised Learning\n",
    "Reinforcement Learning.\n",
    "5. Can you explain what a well-posed learning problem is? Explain the main characteristics that must be present to identify a learning problem properly ?\n",
    "Ans: Well Posed Learning Problem – A computer program is said to learn from experience E in context to some task T and some performance measure P, if its performance on T, as was measured by P, upgrades with experience E.Any problem can be segregated as well-posed learning problem if it has three traits – Task, Performance and Experience.\n",
    "\n",
    "6. Is machine learning capable of solving all problems? Give a detailed explanation of your answer ?\n",
    "Ans: Machine Learning probably can run into all problems for solving but lays down with some concerns which are:\n",
    "\n",
    "Ethics: The idea of trusting data and algorithms more than our own judgment has its pros and cons.Obviously, we benefit from these algorithms, otherwise, we wouldn’t be using them in the first place. These algorithms allow us to automate processes by making informed judgments using available data. Sometimes, however, this means replacing someone’s job with an algorithm, which comes with ethical ramifications.\n",
    "\n",
    "Deterministic Problem: Machine learning is stochastic, not deterministic. A neural network does not understand Newton’s second law, or that density cannot be negative — there are no physical constraints. \n",
    "    \n",
    "Lack of Data: Many machine learning algorithms require large amounts of data before they begin to give useful results.\n",
    "\n",
    "7. What are the various methods and technologies for solving machine learning problems? Any two of them should be defined in detail ?\n",
    "Ans: The Various Technologies Used in Machine Learning Problems are: Scikit Learn, Pytorch, Tensorflow, Keras, Python.\n",
    "\n",
    "Scikit Learn: Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, Regression, clustering and dimensionality reduction via a consistence interface in Python.\n",
    "Tensorflow: It is a open source artificial intelligence library, Using data flow graphs to build models. It allows developers to create large-scale neural networks with many layers. TensorFlow is mainly used for: Classification, Perception, Understanding, Discovering, Prediction and Creation.\n",
    "The Various Methods used in Machine Learning Problems are: Regression, Classification, Clustering, Dimensionality Reductio, Ensemble Methods, Neural Network, Deep Learning, Transfer Learning, Reinforcement Learning, Natural Language Processing, Word Embeddings.\n",
    "\n",
    "Regression methods fall within the category of supervised ML. They help to predict or explain a particular numerical value based on a set of prior data, for example predicting the price of a property based on previous pricing data for similar properties.\n",
    "\n",
    "Another class of supervised ML, classification methods predict or explain a class value. For example, they can help predict whether or not an online customer will buy a product. The output can be yes or no, buyer or not buyer. But classification methods aren’t limited to two classes. For example, a classification method could help to assess whether a given image contains a car or a truck. In this case, the output will be 3 different values:\n",
    "\n",
    "The image contains a car\n",
    "The image contains a truck\n",
    "The image contains neither a car nor a truck.\n",
    "8. Can you explain the various forms of supervised learning? Explain each one with an example application ?\n",
    "Ans The various forms of supervised learning are explained in detail below:\n",
    "\n",
    "Regression: In regression, a single output value is produced using training data.For example, regression can help predict the price of a house based on its locality, size, etc.\n",
    "Classification: It involves grouping the data into classes.eg. If you are thinking of extending credit to a person, you can use classification to determine whether or not a person would be a loan defaulter.\n",
    "Naive Bayesian Model: The Bayesian model of classification is used for large finite datasets. It is a method of assigning class labels using a direct acyclic graph.\n",
    "Decision Trees: A decision tree is a flowchart-like model that contains conditional control statements, comprising decisions and their probable consequences. The output relates to the labelling of unforeseen data.\n",
    "    \n",
    "9. What is the difference between supervised and unsupervised learning? With a sample application in each region, explain the differences ?\n",
    "Ans: Supervised earning algorithms are trained using labeled data. Unsupervised learning algorithms are trained using unlabeled data.In unsupervised learning, only input data is provided to the model.\n",
    "Examples:\n",
    "Supervised Learning : Classification and Regression.\n",
    "Unsuperised Learning : Clustering.\n",
    "\n",
    "10. Describe the machine learning process in depth ?\n",
    "Make brief notes on any two of the following:\n",
    "MATLAB is one of the most widely used programming languages.\n",
    "Deep learning applications in healthcare\n",
    "Study of the market basket\n",
    "Linear regression (simple)\n",
    "Ans: Imagine a dataset as a table, where the rows are each observation (aka measurement, data point, etc), and the columns for each observation represent the features of that observation and their values.At the outset of a machine learning project, a dataset is usually split into two or three subsets. The minimum subsets are the training and test datasets, and often an optional third validation dataset is created as well. Once these data subsets are created from the primary dataset, a predictive model or classifier is trained using the training data, and then the model’s predictive accuracy is determined using the test data. As, machine learning leverages algorithms to automatically model and find patternsin data, usually with the goal of predicting some target output or response. These algorithms are heavily based on statistics and mathematical optimization. Optimization is the process of finding the smallest or largest value (minima or maxima) of a function, often referred to as a loss, or cost function in the minimization case. One of the most popular optimization algorithms used in machine learning is called gradient descent, and another is known as the the normal equation. In a nutshell, machine learning is all about automatically learning a highly accurate predictive or classifier model, or finding unknown patterns in data, by leveraging learning algorithms and optimization techniques.\n",
    "\n",
    "Deep learning applications in healthcare: Deep learning provides the healthcare industry with the ability to analyze data at exceptional speeds without compromising on accuracy. It’s not machine learning, nor is it AI, it’s an elegant blend of both that uses a layered algorithmic architecture to sift through data at an astonishing rate. The benefits of deep learning in healthcare are plentiful – fast, efficient, accurate – but they don’t stop there. Even more benefits lie within the neural networks formed by multiple layers of AI and ML and their ability to learn. Yes, the secret to deep learning’s success is in the name – learning.\n",
    "\n",
    "Linear regression (simple): Linear Regression models describe the relationship between variables by fitting a line to the observed data. Linear regression models use a straight line, while logistic and nonlinear regression models use a curved line. Regression allows you to estimate how a dependent variable changes as the independent variable(s) change\n",
    "    \n",
    "11. Make a comparison between:-\n",
    "Generalization and abstraction\n",
    "Learning that is guided and unsupervised\n",
    "Regression and classification\n",
    "Ans: The differences between among the given concepts is:\n",
    "\n",
    "Generalization and abstraction: Abstraction is the process of removing details of objects. And Generalization, then, is the formulation of general concepts from specific instances by abstracting common properties. A concrete object can be looked at as a “subset” of a more generalized object.\n",
    "\n",
    "Learning that is guided and Unsupervised: Supervised learning is the method that trains machines to use data that is well classified and labeled. Whereas Unsupervised learning, on the other hand, is the method that trains machines to use data that is neither classified nor labeled.\n",
    "\n",
    "Regression and classification:\n",
    "\n",
    "Classification Models – Classification models are used for problems where the output variable can be categorized, such as Yes or No, or Pass or Fail. Classification Models are used to predict the category of the data. Real-life examples include spam detection, sentiment analysis, scorecard prediction of exams, etc.\n",
    "\n",
    "Regression Models – Regression models are used for problems where the output variable is a real value such as a unique number, dollars, salary, weight or pressure, for example. It is most often used to predict numerical values based on previous data observations. Some of the more familiar regression algorithms include linear regression, logistic regression, polynomial regression, and ridge regression.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are the key tasks involved in getting ready to work with machine learning modeling ?\n",
    "Ans: The Key Tasks involved in getting ready to work with Machine learning Modelling are:\n",
    "\n",
    "Data collection: Defining the problem and assembling a dataset.\n",
    "Data preparation: Preparing your data.\n",
    "Choosing a Model\n",
    "Training the Model: Developing a model that does better than a baseline.\n",
    "Evaluating the Model: Choosing a measure of success. Deciding on an evaluation protocol.\n",
    "Parameter tuning: Scaling up: developing a model that overfits.Regularizing your model and tuning your parameters.\n",
    "Prediction or Inference.\n",
    "\n",
    "\n",
    "2. What are the different forms of data used in machine learning ? Give a specific example for each of them ?\n",
    "Ans: Most data can be categorized into 4 basic types from a Machine Learning perspective: Numerical Data, Categorical Data, Time-Series Data, and Text data.\n",
    "        \n",
    "Numerical Data:\n",
    "Numerical data is any data where data points are exact numbers. Statisticians also might call numerical data, quantitative data. This data has meaning as a measurement such as house prices or as a count, such as a number of residential properties in Los Angeles or how many houses sold in the past year.\n",
    "\n",
    "Numerical data can be characterized by continuous or discrete data. Continuous data can assume any value within a range whereas discrete data has distinct values.\n",
    "\n",
    "\n",
    "\n",
    "Categorical Data:\n",
    "Categorical data represents characteristics, such as a hockey player’s position, team, hometown. Categorical data can take numerical values. For example, maybe we would use 1 for the colour red and 2 for blue. But these numbers don’t have a mathematical meaning. That is, we can’t add them together or take the average.\n",
    "\n",
    "\n",
    "Time Series Data:\n",
    "Time series data is a sequence of numbers collected at regular intervals over some period of time. It is very important, especially in particular fields like finance. Time series data has a temporal value attached to it, so this would be something like a date or a timestamp that you can look for trends in time.\n",
    "\n",
    "\n",
    "\n",
    "Text:\n",
    "Text data is basically just words. A lot of the time the first thing that you do with text is you turn it into numbers using some interesting functions like the bag of words formulation.\n",
    "\n",
    "\n",
    "\n",
    "3. Distinguish between :\n",
    "Numeric vs. categorical attributes\n",
    "Feature selection vs. dimensionality reduction\n",
    "Ans: The following are the differences between:\n",
    "\n",
    "Numeric vs. categorical attributes:\n",
    "Numerical data are values obtained for quantitative variable, and carries a sense of magnitude related to the context of the variable (hence, they are always numbers or symbols carrying a numerical value).\n",
    "Categorical data are values obtained for a qualitative variable. categorical data numbers do not carry a sense of magnitude.\n",
    "Numerical data always belong to either ordinal, ratio, or interval type, whereas categorical data belong to nominal type. - - Methods used to analyse quantitative data are different from the methods used for categorical data, even if the principles are the same at least the application has significant differences.\n",
    "Numerical data are analysed using statistical methods in descriptive statistics, regression, time series and many more.For categorical data usually descriptive methods and graphical methods are employed. Some non-parametric tests are also used.\n",
    "Feature selection vs. dimensionality reduction\n",
    "Feature selection you just select a subset of the original feature set, without any manipulation of the data on the other hand.\n",
    "Dimensionality reduction is typically choosing a new representation within which you can describe most but not all of the variance within your data, thereby retaining the relevant information, while reducing theamount of information necessary to represent it.\n",
    "4. Make quick notes on any two of the following ?\n",
    "The histogram\n",
    "Use a scatter plot\n",
    "PCA (Personal Computer Aid)\n",
    "Ans: The Quick notes on the following three topics is:\n",
    "\n",
    "The histogram: A Histogram is a graphical representation that organizes a group of data points into user-specified ranges. Similar in appearance to a bar graph, the histogram condenses a data series into an easily interpreted visual by taking many data points and grouping them into logical ranges or bins.\n",
    "\n",
    "Use a scatter plot: A scatter plot (aka scatter chart, scatter graph) uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables.\n",
    "\n",
    "PCA (Personal Computer Aid): Principal Component Analysis or PCA is a widely used technique for dimensionality reduction of the large data set. Reducing the number of components or features costs some accuracy and on the other hand, it makes the large data set simpler, easy to explore and visualize.\n",
    "\n",
    "5. Why is it necessary to investigate data? Is there a discrepancy in how qualitative and quantitative data are explored ?\n",
    "\n",
    "Ans: If your data set is messy, building models will not help you to solve your problem. What will happen is Garbage In, Garbage Out. In order to build a powerful machine learning algorithm. We need to explore and understand our data set before we define a predictive task and solve it.\n",
    "\n",
    "6. What are the various histogram shapes? What exactly are ‘bins' ?\n",
    "Ans: The different types of a Histogram are:\n",
    "\n",
    "Uniform Histogram\n",
    "Symmetric Histogram\n",
    "Bimodal Histogram\n",
    "Probability Histogram.\n",
    "The bin in a histogram is the choice of unit and spacing on the X-axis. All the data in a probability distribution represented visually by a histogram is filled into the corresponding bins. The height of each bin is a measurement of the frequency with which data appears inside the range of that bin in the distribution.\n",
    "\n",
    "7. How do we deal with data outliers ?\n",
    "Ans: We can use Z-Score or any of below methods to deal with data outliers:\n",
    "\n",
    "Univariate Method: This method looks for data points with extreme values on one variable.\n",
    "\n",
    "Multivariate Method: Here, we look for unusual combinations of all the variables.\n",
    "\n",
    "Minkowski Error: This method reduces the contribution of potential outliers in the training process.\n",
    "\n",
    "Z-Score: This can be done with just one line code as we have already calculated the Z-score.\n",
    "boston_df_o = boston_df_o[(z < 3).all(axis=1)]\n",
    "\n",
    "IQR Score: Calculate IQR score to filter out the outliers by keeping only valid values.\n",
    "boston_df_out = boston_df_o1[~((boston_df_o1 < (Q1 - 1.5 * IQR)) |(boston_df_o1 > (Q3 + 1.5 *IQR))).any(axis=1)]\n",
    "boston_df_out.shape\n",
    "\n",
    "Quantile function: Use quantile() to remove amount of data.\n",
    "\n",
    "8. What are the various central inclination measures? Why does mean vary too much from median in certain data sets ?\n",
    "Ans: Mean, Median and Mode are Central Inclination Measures. Mean varies more than Median due to presence of outliers, as mean is averaging all points while median in like finding a middle number.\n",
    "\n",
    "9. Describe how a scatter plot can be used to investigate bivariate relationships. Is it possible to find outliers using a scatter plot ?\n",
    "\n",
    "Ans: A Scatter Plot (aka scatter chart, scatter graph) uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. So this visualization gives us the idea of bivariate relationship.\n",
    "\n",
    "Scatter plot can also help finding outliers as outliers can be visualized at farther distance than regular data.\n",
    "\n",
    "10. Describe how cross-tabs can be used to figure out how two variables are related ?\n",
    "Ans: Cross tabulation is a method to quantitatively analyze the relationship between multiple variables. Also known as contingency tables or cross tabs, cross tabulation groups variables to understand the correlation between different variables. It also shows how correlations change from one variable grouping to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are the key tasks that machine learning entails? What does data pre-processing imply ?\n",
    "Ans: There are Five core tasks in the common ML workflow:\n",
    "\n",
    "Get Data: The first step in the Machine Learning process is getting data.\n",
    "Cleaning, Preparing & Manipulating Data: Real-world data often has unorganized, missing, or noisy elements.\n",
    "Train Model: This step is where the magic happens!\n",
    "Testing Model.\n",
    "Improving model.\n",
    "Data preprocessing involves transforming raw data to well-formed data sets so that data mining analytics can be applied.\n",
    "Preprocessing involves both data validation and data imputation\n",
    "The Goal of Data Validation is to assess whether the data in question is both complete and accurate.\n",
    "The Goal of Data Imputation is to correct errors and input missing values, Either Manually or Automatically through business process automation (BPA) programming.\n",
    "2. Describe quantitative and qualitative data in depth. Make a distinction between the two ?\n",
    "Ans: The Data Type Is Broadly Classified Into:\n",
    "\n",
    "Quantitative\n",
    "Qualitative\n",
    "Quantitative Data Type: This Type Of Data Type Consists Of Numerical Values. Anything Which Is Measured By Numbers. E.G., Profit, Quantity Sold, Height, Weight, Temperature, Etc. This Is Again Of Two Types\n",
    "Discrete Data Type: – The Numeric Data Which Have Discrete Values Or Whole Numbers. This Type Of Variable Value If Expressed In Decimal Format Will Have No Proper Meaning. Their Values Can Be Counted. E.G.: – No. Of Cars You Have, No. Of Marbles In Containers, Students In A Class, Etc.\n",
    "Continuous Data Type: – The Numerical Measures Which Can Take The Value Within A Certain Range. This Type Of Variable Value If Expressed In Decimal Format Has True Meaning. Their Values Can Not Be Counted But Measured. The Value Can Be Infinite E.G.: Height, Weight, Time, Area, Distance, Measurement Of Rainfall, Etc.\n",
    "Qualitative Data Type: These Are The Data Types That Cannot Be Expressed In Numbers. This Describes Categories Or Groups And Is Hence Known As The Categorical Data Type. This Can Be Divided Into:-\n",
    "Structured Data: This Type Of Data Is Either Number Or Words. This Can Take Numerical Values But Mathematical Operations Cannot Be Performed On It. This Type Of Data Is Expressed In Tabular Format. E.G.) Sunny=1, Cloudy=2, Windy=3 Or Binary Form Data Like 0 Or1, Good Or Bad, Etc.\n",
    "Unstructured Data: This Type Of Data Does Not Have The Proper Format And Therefore Known As Unstructured Data.This Comprises Textual Data, Sounds, Images, Videos, Etc.\n",
    "3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types ?\n",
    "Ans: The following is a basic data collection that includes some sample records.\n",
    "    \n",
    "Determine What Information You Want to Collect: The first thing you need to do is choose what details you want to collect. You’ll need to decide what topics the information will cover, who you want to collect it from and how much data you need. Your goals — what you hope to accomplish using your data — will determine your answers to these questions. As an example, you may decide to collect data about which type of articles are most popular on your website among visitors who are between the ages of 18 and 34. You might also choose to gather information about the average age of all of the customers who bought a product from your company within the last month.\n",
    "\n",
    "Set a Timeframe for Data Collection: Next, you can start formulating your plan for how you’ll collect your data. In the early stages of your planning process, you should establish a timeframe for your data collection. You may want to gather some types of data continuously. When it comes to transactional data and website visitor data, for example, you may want to set up a method for tracking that data over the long term. If you’re tracking data for a specific campaign, however, you’ll track it over a defined period. In these instances, you’ll have a schedule for when you’ll start and end your data collection.\n",
    "\n",
    "Determine Your Data Collection Method: At this step, you will choose the data collection method that will make up the core of your data-gathering strategy. To select the right collection method, you’ll need to consider the type of information you want to collect, the timeframe over which you’ll obtain it and the other aspects you determined.\n",
    "\n",
    "Collect the Data: Once you have finalized your plan, you can implement your data collection strategy and start collecting data. You can store and organize your data in your DMP. Be sure to stick to your plan and check on its progress regularly. It may be useful to create a schedule for when you will check in with how your data collection is proceeding, especially if you are collecting data continuously. You may want to make updates to your plan as conditions change and you get new information.\n",
    "\n",
    "Analyze the Data and Implement Your Findings: Once you’ve collected all of your data, it’s time to analyze it and organize your findings. The analysis phase is crucial because it turns raw data into valuable insights that you can use to enhance your marketing strategies, products and business decisions. You can use the analytics tools built into our DMP to help with this step. Once you’ve uncovered the patterns and insights in your data, you can implement the findings to improve your business.\n",
    "\n",
    "4. What are the various causes of machine learning data issues? What are the ramifications ?\n",
    "Ans: Noisy data, dirty data, and incomplete data are the quintessential enemies of ideal Machine Learning. The solution to this conundrum is to take the time to evaluate and scope data with meticulous data governance, data integration, and data exploration until you get clear data. Ramifications or major issues in machine learning are:\n",
    "\n",
    "Five practical issues in machine learning and the business implications Data quality.\n",
    "Machine learning systems rely on data.\n",
    "The complexity and quality trade-off.\n",
    "Sampling bias in data.\n",
    "Changing expectations and concept drift.\n",
    "Monitoring and maintenance.\n",
    "5. Demonstrate various approaches to categorical data exploration with appropriate examples ?\n",
    "\n",
    "Ans: Various approaches to categorical data exploration are:\n",
    "\n",
    "Unique value count: One of the first things which can be useful during data exploration is to see how many unique values are there in categorical columns.\n",
    "Frequency Count: Frequency count is finding how frequent individual values occur in column.\n",
    "Variance: Variance gives a good indication how the values are spread.\n",
    "Pareto Analysis: Pareto analysis is a creative way of focusing on what is important. Pareto 80–20 rule can be effectively used in data exploration.\n",
    "Histogram: Histogram are one of the data scientists favourite data exploration techniques. It gives information on the range of values in which most of the values fall. It also gives information on whether there is any skew in data.\n",
    "Correlation Heat-map between all numeric columns: The term correlation refers to a mutual relationship or association between two things.\n",
    "Pearson Correlation and Trend between two numeric columns: Once you have visualised correlation heat-map , the next step is to see the correlation trend between two specific numeric columns.\n",
    "Outlier overview: Finding something unusual in data is called Outlier detection (also known as anomaly detection). These outliers represent something unusual, rare , anomaly or something exceptional.\n",
    "6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it ?\n",
    "Ans: Even in a Well-Designed & Controlled study, Missing data occurs in almost all research. Missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions.\n",
    "\n",
    "Real-world data collection has its own set of problems, It is often very messy which includes missing data, presence of outliers, unstructured manner, etc.\n",
    "Before looking for any insights from the data, we have to first perform preprocessing tasks which then only allow us to use that data for further observation and train our machine learning model.\n",
    "Missing value in a dataset is a very common phenomenon in the reality.\n",
    "Missing value correction is required to reduce bias and to produce powerful suitable models.\n",
    "Most of the algorithms can’t handle missing data, thus you need to act in some way to simply not let your code crash. So, let’s begin with the methods to solve the problem.\n",
    "Methods for dealing with missing values. The popular methods which are used by the machine learning community to handle the missing value for categorical variables in the dataset are as follows: Delete the observations: If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model.\n",
    "7. Describe the various methods for dealing with missing data values in depth ?\n",
    "Ans: The Various Methods for dealing with missing data values are:\n",
    "        \n",
    "Delete the observations: If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model. For Example Implement this method in a given dataset, we can delete the entire row which contains missing values.\n",
    "Replace missing values with the most frequent value: You can always impute them based on Mode in the case of categorical variables, just make sure you don’t have highly skewed class distributions.\n",
    "Develop a model to predict missing values: One smart way of doing this could be training a classifier over your columns with missing values as a dependent variable against other features of your data set and trying to impute based on the newly trained classifier.\n",
    "8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words ?\n",
    "Ans: The Various Data Pre-Processing Techniques are:\n",
    "\n",
    "Data Cleaning: The data can have many irrelevant and missing parts. To handle this part, data cleaning is done. It involves handling of missing data, noisy data etc.\n",
    "Missing Data: This situation arises when some data is missing in the data. It can be handled in various ways. Some of them are:\n",
    "Ignore the tuples: This approach is suitable only when the dataset we have is quite large and multiple values are missing within a tuple.\n",
    "Fill the Missing values: There are various ways to do this task. You can choose to fill the missing values manually, by attribute mean or the most probable value.\n",
    "Noisy Data: Noisy data is a meaningless data that can’t be interpreted by machines.It can be generated due to faulty data collection, data entry errors etc. It can be handled in following ways :\n",
    "Binning Method: This method works on sorted data in order to smooth it. The whole data is divided into segments of equal size and then various methods are performed to complete the task. Each segmented is handled separately. One can replace all data in a segment by its mean or boundary values can be used to complete the task.\n",
    "Regression: Here data can be made smooth by fitting it to a regression function.The regression used may be linear (having one independent variable) or multiple (having multiple independent variables).\n",
    "Clustering: This approach groups the similar data in a cluster. The outliers may be undetected or it will fall outside the clusters.\n",
    "Data Reduction: Since data mining is a technique that is used to handle huge amount of data. While working with huge volume of data, analysis became harder in such cases. In order to get rid of this, we uses data reduction technique. It aims to increase the storage efficiency and reduce data storage and analysis costs. The various steps to data reduction are:\n",
    "Data Cube Aggregation: Aggregation operation is applied to data for the construction of the data cube.\n",
    "Attribute Subset Selection: The highly relevant attributes should be used, rest all can be discarded. For performing attribute selection, one can use level of significance and p- value of the attribute.the attribute having p-value greater than significance level can be discarded.\n",
    "Numerosity Reduction: This enable to store the model of data instead of whole data, for example: Regression Models.\n",
    "Dimensionality Reduction: This reduce the size of data by encoding mechanisms.It can be lossy or lossless. If after reconstruction from compressed data, original data can be retrieved, such reduction are called lossless reduction else it is called lossy reduction. The two effective methods of dimensionality reduction are:Wavelet transforms and PCA (Principal Component Analysis).\n",
    "        \n",
    "Feature selection is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension.\n",
    "\n",
    "9.Make brief notes on of the following ?\n",
    "What is the IQR? What criteria are used to assess it?\n",
    "Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers?\n",
    "Ans: The following is the brief notes on the following topics:\n",
    "\n",
    "What is the IQR? What criteria are used to assess it?\n",
    "Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.\n",
    "The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.\n",
    "Describe the various components of a box plot in detail? When will the lower whisker surpass the upper whisker in length? How can box plots be used to identify outliers?\n",
    "minimum is the minimum value in the dataset\n",
    "maximum is the maximum value in the dataset.\n",
    "So the difference between the two tells us about the range of dataset.\n",
    "The median is the median (or centre point), also called second quartile, of the data (resulting from the fact that the data is ordered).\n",
    "Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "Q3 is the third quartile of the data, i.e., to say 75%\n",
    "When the data is left skewed, lower whisker will be longer than upper whisker.\n",
    "To detect the outliers this method is used, we define a new range, let’s call it decision range, and any data point lying outside this range is considered as outlier and is accordingly dealt with. The range is as given below: Lower Bound: (Q1 - 1.5 * IQR)Upper Bound: (Q3 + 1.5 * IQR)\n",
    "The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.\n",
    "\n",
    "10. Make brief notes on any two of the following ?\n",
    "Data collected at regular intervals\n",
    "The gap between the quartiles\n",
    "Use a cross-tab\n",
    "Ans: The following are the breif notes about:\n",
    "\n",
    "Data collected at regular intervals:\n",
    "Interval data is one of the two types of discrete data.\n",
    "An example of interval data is the data collected on a thermometer—its gradation or markings are equidistant.\n",
    "Unlike ordinal data, interval data always take numerical values where the distance between two points on the scale is standardised and equal.\n",
    "The gap between the quartiles:\n",
    "Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3.\n",
    "The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.\n",
    "11. Make a comparison between ?\n",
    "Data with nominal and ordinal values\n",
    "Histogram and box plot\n",
    "The average and median\n",
    "Ans: The following are the breif notes about:\n",
    "\n",
    "The average and median:\n",
    "The mean (informally, the “average“) is found by adding all of the numbers together and dividing by the number of items in the set: 10 + 10 + 20 + 40 + 70 / 5 = 30. The median is found by ordering the set from lowest to highest and finding the exact middle. The median is just the middle number: 20\n",
    "Histogram and barplot:\n",
    "Histograms and box plots are very similar in that they both help to visualize and describe numeric data. Although histograms are better in determining the underlying distribution of the data, box plots allow you to compare multiple data sets better than histograms as they are less detailed and take up less space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. In the sense of machine learning, what is a model? What is the best way to train a model?\n",
    "Ans: A Machine Learning Model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data.\n",
    "\n",
    "Model Naming — Give Your Model a Name, Let’s start with giving your model a name, describe your model and attach tags to your model. Tags are to make your model searchable.\n",
    "Data Type Selection — Choose data type(Images/Text/CSV), It’s time to tell us about the type of data you want to train your model. ML Models support Images, Text and *.CSV (categorical data) data types.\n",
    "Data Upload — Upload your data or choose from Public Data Sets: Choose from public datasets like Jewellery Data set (Images), Gender Data Set (Images), Question or Sentence Data Set (Text), Numerai Data Set (CSV) or upload your data.\n",
    "Type category(label) for the files (images/text file) that you have uploaded and click on submit to begin upload. Wait for some time till our web app uploads all the files. You can upload images for as many categories as possible.\n",
    "Start Training - Push the button, to start the training. Now Mateverse’s intelligent backend will start with processing the data that you have uploaded and preparing it for the training.\n",
    "2. In the sense of machine learning, explain the \"No Free Lunch\" theorem.\n",
    "Ans: The No Free Lunch Theorem, often abbreviated as NFL or NFLT, is a theoretical finding that suggests all optimization algorithms perform equally well when their performance is averaged over all possible objective functions. In computational complexity and optimization the no free lunch theorem is a result that states that for certain types of mathematical problems, the computational cost of finding a solution, averaged over all problems in the class, is the same for any solution method.\n",
    "\n",
    "In Simple Words “No Free Lunch” theorem means we can’t rely on one model to be best of all models. We have to understand data properly and make use of ML understanding and make use of models to find best out of it.\n",
    "\n",
    "3. Describe the K-fold cross-validation mechanism in detail.\n",
    "Ans: In K-fold cross validation, data D is subset into k subsets randomly. Let us assume S1...Sk are the subsets where Sk is the kth randomly split subset of data D. In the first iteration, D-S1 is used for training and S1 for testing the model. When the model has been trained and tested, evaluation can be done, score is noted elsewhere and the trained model is discarded.\n",
    "\n",
    "These k-iterations go on where 1/k subset of D is always set aside for testing the data and D-1/k subsets are used for training, evaluating and discarding the model. At the end of all the iterations, average of all the evaluation scores is taken and used as output.\n",
    "\n",
    "4. Describe the bootstrap sampling method. What is the aim of it?\n",
    "Ans: The bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples.\n",
    "\n",
    "Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen. This allows a given observation to be included in a given small sample more than once. This approach to sampling is called sampling with replacement.\n",
    "\n",
    "5. What is the significance of calculating the Kappa value for a classification model? Demonstrate how to measure the Kappa value of a classification model using a sample collection of results.\n",
    "Ans: Kappa value or Cohen's Kappa coefficient is an evaluation metric for classification models. Its significance as an evaluation metric is that it can be used to evaluate multi class classification models and also works on models trained on imbalanced datasets(scores like accuracy scores fail for imbalanced datasets).\n",
    "\n",
    "In simpler words It basically tells you how much better your classifier is performing over the performance of a classifier that simply guesses at random according to the frequency of each class. Cohen's kappa is always less than or equal to 1. Values of 0 or less, indicate that the classifier is useless Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.\n",
    "\n",
    "6. Describe the model ensemble method. In machine learning, what part does it play?\n",
    "Ans: Ensemble methods or ensemble machine learning models are models where more than one models are being used spontaneously to produce better results than individually trained models.\n",
    "\n",
    "7. What is a descriptive model's main purpose? Give examples of real-world problems that descriptive models were used to solve.\n",
    "Ans: A descriptive model is used for tasks that would benefit from the insight gained from summarizing data in new and interesting ways. As opposed to predictive models that predict a target of interest, in a descriptive model, no single feature is more important than any other. In fact, because there is no target to learn, the process of training a descriptive model is called unsupervised learning.\n",
    "\n",
    "It is used in customer classification as real life problem .\n",
    "\n",
    "8. Describe how to evaluate a linear regression model.\n",
    "Ans: Evaluation of a linear regression model can be done using R-square. R square is calculated as the sum of squared errors in predictions made, divided by summation of all sum of squares. R square measures how much of the change in target variable can be explained by the linear regressor. Its value ranges from 0 to 1 where 0 means poor performance and 1 means good. Some other techniques which can be used to evaluate a linear regression model are:\n",
    "\n",
    "Mean Square Error(MSE)/Root Mean Square Error(RMSE)\n",
    "Mean Absolute Error(MAE)\n",
    "9. Distinguish :\n",
    "Descriptive vs. predictive models\n",
    "Underfitting vs. overfitting the model\n",
    "Bootstrapping vs. cross-validation\n",
    "Ans: The differences between:\n",
    "\n",
    "Descriptive vs. predictive models\n",
    "Descriptive models are built to identify trends and underlying patterns.\n",
    "Predictive models are built to predict a dependent variable value.\n",
    "Most of descriptive models are built using unsupervised machine learning.\n",
    "Most of predictive models are built using classification and regression models.\n",
    "Example for descriptive model: Finding why consumers are engaging more with a social media post.\n",
    "Example for predictive model: Predicting the chances of cancer in a patient.\n",
    "Underfitting vs. overfitting the model\n",
    "Underfitting is a situation arising when the hypothesis is way too simple, or when the machine learning model is way too simple to produce good results.\n",
    "Overfitting is a situation arising when the hypothesis is way too complex, or when the machine learning model is way too complex to produce good results.\n",
    "Underfitting causes a model to produce poor results due to heavily simplified algorithm reacting lightly to changes in the unseen data for independent variables from the training data.\n",
    "Overfitting makes a model produce poor results due to slightest variations in the unseen data for independent variables from the training data\n",
    "Underfitting is also called High Bias.\n",
    "Overfitting is also called High variance\n",
    "Bootstrapping vs cross-validation\n",
    "Boostrap sampling is a method of sampling in which the repeated sampling is done with replacement using a data D in random draws over which machine learning models are trained for better performance.\n",
    "Cross validation is a method used to check the efficacy of the machine learning model on test data.\n",
    "End goal of bootstrapping is to reduce overfitting and increase performance.\n",
    "End goal of cross validation is only to produce test scores to check efficacy of model\n",
    "Bootstrapping is best employed in Random Forest Classifier.\n",
    "Cross Validation is best employed using K-fold cross validation technique.\n",
    "\n",
    "10. Make quick notes on:\n",
    "LOOCV.\n",
    "F-measurement\n",
    "The width of the silhouette\n",
    "Receiver operating characteristic curve\n",
    "Ans: The Quick notes on: LOOCV or Leave One Out Cross Validation is a form of K-fold cross validation where only one observation is left out for validation purpose while the rest of the data is used for model training each iteration. It is computationally taxing and should only be used for data with low dimensionality.\n",
    "\n",
    "Harmonic mean of Precision score and recall score is called F-measurement or F-score. It is formulated as 2 (pr re)/pr +re where pr is precision score and re is recall score.\n",
    "\n",
    "Estimate of average inter cluster distance to give efficacy/performance of cluster algorithms is called width of the silhouette. It can also be defined as how identical/similar a data point 'x' is to the data points inside the cluster to which x is assigned. Its value ranges from -1 to 1 where 1 means good and -1 means bad.\n",
    "\n",
    "Curve plotted between True Positive Rate and False Positive Rate is Receiver Operating Characteristics curve and is used to find the area under the curve for ROC-AUC score for binary classification evaluation. True Positive Rate and False Positive Rate are calculated for different thresholds values where thresholds take values starting from the highest probability scores assigned to data points and goes up to the lowest probability score. The curve is impacted by presence of outliers, and simple models. Extensions can be made to this curve to suit multiclass classification evaluation requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the definition of a target function ? In the sense of a real-life example, express the target function. How is a target function's fitness assessed ?\n",
    "Ans: A target function, in machine learning, is a method for solving a problem that an AI algorithm parses its training data to find.The target function is essentially the formula that an algorithm feeds data to in order to calculate predictions.\n",
    "\n",
    "Analyzing the massive amounts of data related to its given problem, an AI derives understanding of previously unspecified rules by detecting consistencies in the data. The observations of inherent rules about how the studied subject operates inform the AI on how to process future data that does not include an output by applying this previously unknown function.\n",
    "\n",
    "2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models ?\n",
    "Ans: In short, predictive modeling is a statistical technique using machine learning and data mining to predict and forecast likely future outcomes with the aid of historical and existing data.\n",
    "\n",
    "It works by analyzing current and historical data and projecting what it learns on a model generated to forecast likely outcomes.\n",
    "\n",
    "The three main types of descriptive studies are Case studies, Naturalistic observation, and Surveys.\n",
    "\n",
    "Some examples of descriptive research are: A specialty food group launching a new range of barbecue rubs would like to understand what flavors of rubs are favored by different people.\n",
    "\n",
    "Case Studies are a type of observational research that involve a thorough descriptive analysis of a single individual, group, or event. There is no single way to conduct a case study so researchers use a range of methods from unstructured interviewing to direct observation.\n",
    "\n",
    "3. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters ?\n",
    "Ans: Logarithmic loss (or log loss) measures the performance of a classification model where the prediction is a probability value between 0 and 1.\n",
    "\n",
    "Log loss increases as the predicted probability diverge from the actual label. Log loss is a widely used metric for Kaggle competitions. Input on the most important basics for the measurement of the physical parameters: Temperature, flow velocity, humidity, pressure, CO2 and infrared. Tips on correct measurement and for avoiding measurement errors.\n",
    "\n",
    "4. Describe :\n",
    "In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting ?\n",
    "What does it mean to overfit? When is it going to happen?\n",
    "In the sense of model fitting, explain the bias-variance trade-off.\n",
    "Ans: The following is the short notes on:\n",
    "\n",
    "In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting:\n",
    "\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error\n",
    "rate on both the training set and unseen data.\n",
    "\n",
    "What does it mean to overfit? When is it going to happen\n",
    "\n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model.\n",
    "\n",
    "In the sense of model fitting, explain the bias-variance trade-off\n",
    "\n",
    "The bias is known as the difference between the prediction of the values by the ML model and the correct value. Being high in biasing gives a large error in training as well as testing data. By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set.\n",
    "\n",
    "5. Is it possible to boost the efficiency of a learning model? If so, please clarify how ?\n",
    "Ans: Building a machine learning model is not enough to get the right predictions, as you have to check the accuracy and need to validate the same to ensure get the precise results. And validating the model will improve the performance of the ML model.\n",
    "\n",
    "6. How would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model ?\n",
    "Ans: In case of supervised learning, it is mostly done by measuring the performance metrics such as accuracy, precision, recall, AUC, etc. on the training set and the holdout sets.\n",
    "\n",
    "Few examples of such measures are:\n",
    "\n",
    "Silhouette coefficient.\n",
    "Calisnki-Harabasz coefficient.\n",
    "Dunn index.\n",
    "Xie-Beni score.\n",
    "Hartigan index.\n",
    "7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer ?\n",
    "Ans: Categorical Data is the data that generally takes a limited number of possible values. Also, the data in the category need not be numerical, it can be textual in nature. All machine learning models are some kind of mathematical model that need numbers to work with. This is one of the primary reasons we need to pre-process the categorical data before we can feed it to machine learning models.\n",
    "\n",
    "If a categorical target variable needs to be encoded for a classification predictive modeling problem, then the LabelEncoder class can be used.\n",
    "\n",
    "8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling ?\n",
    "Ans: predictive modeling is a statistical technique using machine learning and data mining to predict and forecast likely future outcomes with the aid of historical and existing data. It works by analyzing current and historical data and projecting what it learns on a model generated to forecast likely outcomes.\n",
    "\n",
    "Classification is the process of identifying the category or class label of the new observation to which it belongs.Predication is the process of identifying the missing or unavailable numerical data for a new observation. That is the key difference between classification and prediction.\n",
    "\n",
    "9. Make quick notes on:\n",
    "The process of holding out\n",
    "Cross-validation by tenfold\n",
    "Adjusting the parameters\n",
    "Ans: The Quick notes on the following topics is below:\n",
    "\n",
    "The process of holding out:\n",
    "The hold-out method for training machine learning model is the process of splitting the data in different splits and using one split for training the model and other splits for validating and testing the models. The hold-out method is used for both model evaluation and model selection.\n",
    "Cross-validation by tenfold: 10-fold cross validation would perform the fitting procedure a total of ten times, with each fit being performed on a training set consisting of 90% of the total training set selected at random, with the remaining 10% used as a hold out set for validation.\n",
    "Adjusting the parameters:\n",
    "A fancy name for training: the selection of parameter values, which are optimal in some desired sense (eg. minimize an objective function you choose over a dataset you choose). The parameters are the weights and biases of the network\n",
    "10. Define the following terms:\n",
    "Purity vs. Silhouette width\n",
    "Boosting vs. Bagging\n",
    "The eager learner vs. the lazy learner\n",
    "Ans: The Following is the short notes on:\n",
    "\n",
    "Purity vs Silhouette width:\n",
    "Purity is a measure of the extent to which clusters contain a single class. Its calculation can be thought of as follows: For each cluster, count the number of data points from the most common class in said cluster.\n",
    "The silhouette width is also an estimate of the average distance between clusters. Its value is comprised between 1 and -1 with a value of 1 indicating a very good cluster.\n",
    "Boosting vs. Bagging:\n",
    "Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data.\n",
    "Boosting is an iterative technique which adjusts the weight of an observation based on the last classification.\n",
    "The eager learner vs. the lazy learner:\n",
    "A lazy learner delays abstracting from the data until it is asked to make a prediction.\n",
    "while an eager learner abstracts away from the data during training and uses this abstraction to make predictions rather than directly compare queries with instances in the dataset.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What exactly is a feature? Give an example to illustrate your point ?\n",
    "Ans: Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning.\n",
    "\n",
    "Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project.\n",
    "\n",
    "2. What are the various circumstances in which feature construction is required ?\n",
    "Ans: The features in your data will directly influence the predictive models you use and the results you can achieve. Your results are dependent on many inter-dependent properties. You need great features that describe the structures inherent in your data. Better features means flexibility.\n",
    "\n",
    "3. Describe how nominal variables are encoded ?\n",
    "Ans: Nominal data is made of discrete values with no numerical relationship between the different categories — mean and median are meaningless. Animal species is one example. For example, pig is not higher than bird and lower than fish.\n",
    "\n",
    "4. Describe how numeric features are converted to categorical features ?\n",
    "Ans: Converting categorical features into numeric features using domain knowledge. For example, we are given a list of countries and say we know the distance to these countries from India then we can replace it with distance from India. So, every country can be represented as its distance from India.\n",
    "\n",
    "5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach ?\n",
    "Ans: Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy\n",
    "\n",
    "6. When is a feature considered irrelevant? What can be said to quantify it ?\n",
    "Ans: Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise.\n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "Wrapper methods (forward, backward, and stepwise selection)\n",
    "Filter methods (ANOVA, Pearson correlation, variance thresholding)\n",
    "Embedded methods (Lasso, Ridge, Decision Tree).\n",
    "7. When is a function considered redundant? What criteria are used to identify features that could be redundant ?\n",
    "Ans: If two features {X1, X2} are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features.\n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes\n",
    "\n",
    "8. What are the various distance measurements used to determine feature similarity ?\n",
    "Ans: Four of the most commonly used distance measures in machine learning are as follows:\n",
    "\n",
    "Hamming Distance.\n",
    "Euclidean Distance\n",
    "Manhattan Distance.\n",
    "9. State difference between Euclidean and Manhattan distances ?\n",
    "Ans: Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences. Euclidean distance is extensively applied in analysis of convolutional codes and Trellis codes.\n",
    "\n",
    "Hamming distance is frequently encountered in the analysis of block codes.\n",
    "\n",
    "10. Distinguish between feature transformation and feature selection ?\n",
    "Ans: Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
    "Ans: Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning. In order to make machine learning work well on new tasks, it might be necessary to design and train better features.\n",
    "\n",
    "Feature engineering in ML consists of four main steps: Feature Creation, Transformations, Feature Extraction, and Feature Selection. Feature engineering consists of creation, transformation, extraction, and selection of features, also known as variables, that are most conducive to creating an accurate ML algorithm.\n",
    "\n",
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n",
    "Ans: Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n",
    "\n",
    "There are three types of feature selection:\n",
    "\n",
    "Wrapper methods (forward, backward, and stepwise selection)\n",
    "Filter methods (ANOVA, Pearson correlation, variance thresholding)\n",
    "Embedded methods (Lasso, Ridge, Decision Tree).\n",
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    "Ans: The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "The filter method has the fastest running time; however, it does not consider feature dependencies and tends to each feature separately when univariate techniques are used. The wrapper method has the advantages of better generalization and robust interaction with the classifier used for feature selection\n",
    "\n",
    "4. Please Answer the following Questions :\n",
    "Describe the overall feature selection process.\n",
    "Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    "Ans: Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "\n",
    "5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    "Ans: Text classification is the problem of assigning categories to text data according to its content. The most important part of text classification is feature engineering: the process of creating features for a machine learning model from raw text data.\n",
    "\n",
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n",
    "Ans: Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space. It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes).\n",
    "\n",
    "7. Explain the following:\n",
    "What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "Compare the Jaccard index and similarity matching coefficient of two features with values (1,1,0,0,1,0,1,1) and (1,1,0,0, 0,1,1,1), respectively (1,0,0,1,1,0,0,1).\n",
    "Ans: Thus the Hamming distance between two vectors is the number of bits we must change to change one into the other. Example Find the distance between the vectors 01101010 and 11011011. They differ in four places, so the Hamming distance d(01101010,11011011) = 4.\n",
    "\n",
    "8. State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "Ans: High dimension is when variable numbers p is higher than the sample sizes n i.e. p>n, cases. High dimensional data is referred to a data of n samples with p features, where p is larger than n.\n",
    "\n",
    "For example, tomographic imaging data, ECG data, and MEG data. One example of high dimensional data is microarray gene expression data.\n",
    "\n",
    "9. Make a few quick notes on:\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "Use of vectors\n",
    "Embedded technique\n",
    "Ans: The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data. The technique is widely used to emphasize variation and capture strong patterns in a data set.\n",
    "\n",
    "Vectors can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows\n",
    "\n",
    "In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well\n",
    "\n",
    "10. Make a comparison between:\n",
    "Sequential backward exclusion vs. sequential forward selection\n",
    "Function selection methods: filter vs. wrapper\n",
    "SMC vs. Jaccard coefficient\n",
    "Ans: Sequential floating forward selection (SFFS) starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases. Sequential floating backward selection (SFBS) starts from the full set.\n",
    "\n",
    "The Jaccard coefficient is a measure of the percentage of overlap between sets defined as: (5.1) where W1 and W2 are two sets, in our case the 1-year windows of the ego networks. The Jaccard coefficient can be a value between 0 and 1, with 0 indicating no overlap and 1 complete overlap between the sets.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Define the Bayesian interpretation of probability.\n",
    "Ans: Bayesian probability is the study of subjective probabilities or belief in an outcome, compared to the frequentist approach where probabilities are based purely on the past occurrence of the event. A Bayesian Network captures the joint probabilities of the events represented by the model.\n",
    "\n",
    "2. Define probability of a union of two events with equation.\n",
    "Ans: The general probability addition rule for the union of two events states that P(A∪B)=P(A)+P(B)−P(A∩B), where A∩B is the intersection of the two sets.\n",
    "\n",
    "3. What is joint probability? What is its formula?\n",
    "Ans: Probabilities are combined using multiplication, therefore the joint probability of independent events is calculated as the probability of event A multiplied by the probability of event B. This can be stated formally as follows:\n",
    "\n",
    "Joint Probability: P(A and B) = P(A)*P(B)\n",
    "\n",
    "4. What is chain rule of probability?\n",
    "Ans: The chain rule, or general product rule, calculates any component of the joint distribution of a set of random variables using only conditional probabilities. This probability theory is used as a foundation for backpropagation and in creating Bayesian networks.\n",
    "\n",
    "5. What is conditional probability means? What is the formula of it?\n",
    "Ans: Conditional probability is defined as the likelihood of an event or outcome occurring, based on the occurrence of a previous event or outcome.\n",
    "\n",
    "Conditional probability is calculated by multiplying the probability of the preceding event by the updated probability of the succeeding, or conditional, event.\n",
    "\n",
    "Let's take a real-life example. Probability of selling a TV on a given normal day may be only 30%. But if we consider that given day is Diwali, then there are much more chances of selling a TV. The conditional Probability of selling a TV on a day given that Day is Diwali might be 70%.\n",
    "\n",
    "6. What are continuous random variables?\n",
    "Ans: A continuous random variable X takes all values in a given interval of numbers. ▪ The probability distribution of a continuous random variable is shown by a density curve. ▪ The probability that X is between an interval of numbers is the area under the density curve between the interval endpoints.\n",
    "\n",
    "7. What are Bernoulli distributions? What is the formula of it?\n",
    "Ans: A Bernoulli distribution is a discrete probability distribution for a Bernoulli trial — a random experiment that has only two outcomes (usually called a Succes or a Failure).The expected value for a random variable, X.\n",
    "\n",
    "For a Bernoulli distribution is: E[X] = p.For example, if p = 0.04, then E[X] = 0.4\n",
    "\n",
    "8. What is binomial distribution? What is the formula?\n",
    "Ans: The binomial is a type of distribution that has two possible outcomes (the prefix “bi” means two, or twice).For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail.\n",
    "\n",
    "A Binomial Distribution shows either (S)uccess or (F)ailure.\n",
    "\n",
    "9. What is Poisson distribution? What is the formula?\n",
    "Ans: A Poisson distribution is defined as a discrete frequency distribution that gives the probability of the number of independent events that occur in the fixed time.\n",
    "\n",
    "In statistics, a Poisson distribution is a probability distribution that is used to show how many times an event is likely to occur over a specified period. ... The Poisson distribution is a discrete function, meaning that the variable can only take specific values in a (potentially infinite) list.\n",
    "\n",
    "10. Define covariance ?\n",
    "Ans: Covariance is a measure of how much two random variables vary together. It's similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together.\n",
    "\n",
    "11. Define correlation ?\n",
    "Ans: Correlation explains how one or more variables are related to each other. These variables can be input data features which have been used to forecast our target variable. It's a bi-variate analysis measure which describes the association between different variables.\n",
    "\n",
    "12. Define sampling with replacement. Give example.\n",
    "Ans: If you sample with replacement, you would choose one person's name, put that person's name back in the hat, and then choose another name. The possibilities for your two-name sample are: John, John. John, Jack.\n",
    "\n",
    "13. What is sampling without replacement? Give example.\n",
    "Ans: In sampling without replacement, each sample unit of the population has only one chance to be selected in the sample. For example, if one draws a simple random sample such that no unit occurs more than one time in the sample, the sample is drawn without replacement.\n",
    "\n",
    "14.What is hypothesis? Give example.\n",
    "Ans: A hypothesis (plural hypotheses) is a proposed explanation for a phenomenon. For a hypothesis to be a scientific hypothesis, the scientific method requires that one can test it. ... Even though the words \"hypothesis\" and \"theory\" are often used synonymously, a scientific hypothesis is not the same as a scientific theory.\n",
    "\n",
    "Examples of hypothesis statements: If garlic repels fleas, then a dog that is given garlic every day will not get fleas. Bacterial growth may be affected by moisture levels in the air. If sugar causes cavities, then people who eat a lot of candy may be more prone to cavities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Given X be a discrete random variable with the following PMF\n",
    "\n",
    "\n",
    "Find the range RX of the random variable X.\n",
    "Find P(X ≤ 0.5)\n",
    "Find P(0.25<X<0.75)\n",
    "P(X = 0.2|X<0.6)\n",
    " \n",
    "2.Two equal and fair dice are rolled, and we observed two numbers X and Y.\n",
    "\n",
    "Find RX, RY, and the PMFs of X and Y.\n",
    "Find P(X = 2,Y = 6).\n",
    "Find P(X>3|Y = 2).\n",
    "If Z = X + Y. Find the range and PMF of Z.\n",
    "Find P(X = 4|Z = 8).\n",
    "\n",
    "3. In an exam, there were 20 multiple-choice questions. Each question had 44 possible options. A student knew the answer to 10 questions, but the other 10 questions were unknown to him, and he chose answers randomly. If the student X's score is equal to the total number of correct answers, then find out the PMF of X. What is P(X>15)?\n",
    " \n",
    "4. The number of students arriving at a college between a time interval is a Poisson random variable. On average, \n",
    "\n",
    "10 students arrive per hour. Let Y be the number of students arriving from 10 am to 11:30 am. What is P(10<Y≤15)?\n",
    " \n",
    "5.Two independent random variables, X and Y,are given such that X~Poisson(α) and Y~Poisson(β). State a new random variable as Z = X + Y. Find out the PMF of Z.\n",
    " \n",
    "6. There is a discrete random variable X with the pmf.\n",
    " \n",
    " If we define a new random variable Y = (X + 1)2 then\n",
    "\n",
    "Find the range of Y.\n",
    "Find the pmf of Y.\n",
    "\n",
    "2.Assuming X is a continuous random variable with PDF\n",
    "\n",
    "\n",
    "Find EX and Var(X).\n",
    "Find P(X ≥ 1/2).\n",
    "\n",
    "If X is a continuous random variable with pdf image-3.png\n",
    "\n",
    "If X~Uniform and Y = sin(X), then find fY(y).\n",
    "If X is a random variable with CDF image-4.png\n",
    "What kind of random variable is X: discrete, continuous, or mixed?\n",
    "Find the PDF of X, fX(x).\n",
    "Find E(eX).\n",
    "Find P(X = 0|X≤0.5).\n",
    " \n",
    "5. There are two random variables X and Y with joint PMF given in Table below\n",
    "Find P(X≤2, Y≤4).\n",
    "Find the marginal PMFs of X and Y.\n",
    "Find P(Y = 2|X = 1).\n",
    "Are X and Y independent?\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "6.A box containing 40 white shirts and 60 black shirts. If we choose 10 shirts (without replacement) at random, find the joint PMF of X and Y, where X is the number of white shirts and Y is the number of black shirts.\n",
    "\n",
    "7.If A and B are two jointly continuous random variables with joint PDF\n",
    "\n",
    "\n",
    "Find fX(a) and fY(b).\n",
    "Are A and B independent of each other?\n",
    "Find the conditional PDF of A given B = b, fA|B(a|b).\n",
    "Find E[A|B = b], for 0 ≤ y ≤ 1.\n",
    "Find Var(A|B = b), for 0 ≤ y ≤ 1.\n",
    " \n",
    "8.There are 100 men on a ship. If Xi is the ith man's weight on the ship and Xi's are independent and identically distributed and EXi = μ = 170 and σXi = σ = 30. Find the probability that the men's total weight on the ship exceeds 18,000.\n",
    " \n",
    "9.Let X1, X2, ……, X25 are independent and identically distributed. And have the following PMF. If Y = X1 + X2 + … + Xn, estimate P(4 ≤ Y ≤ 6) using central limit theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment -12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is prior probability ? Give an example ?\n",
    "Ans: Prior probability shows the likelihood of an outcome in a given dataset. For example, in the mortgage case, P(Y) is the default rate on a home mortgage, which is 2%. P(Y|X) is called the conditional probability, which provides the probability of an outcome given the evidence, that is, when the value of X is known.\n",
    "\n",
    "2. What is posterior probability ? Give an example ?\n",
    "Ans:: Posterior probability is a revised probability that takes into account new available information. For example, let there be two urns, urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls.\n",
    "\n",
    "3. What is likelihood probability ? Give an example ?\n",
    "Ans: Likelihood Function in Machine Learning and Data Science is the joint probability distribution(jpd) of the dataset given as a function of the parameter. Think of it as the probability of obtaining the observed data given the parameter values.\n",
    "\n",
    "4. What is Naïve Bayes classifier ? Why is it named so ?\n",
    "Ans: Naive Bayes is a simple and powerful algorithm for predictive modeling. Naive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data; however, the technique is very effective on a large range of complex problems.\n",
    "\n",
    "5. What is optimal Bayes classifier ?\n",
    "Ans: The Bayes Optimal Classifier is a probabilistic model that makes the most probable prediction for a new example. Bayes Optimal Classifier is a probabilistic model that finds the most probable prediction using the training data and space of hypotheses to make a prediction for a new data instance.\n",
    "\n",
    "6. Write any two features of Bayesian learning methods ?\n",
    "Ans: A probability distribution over observed data for each possible hypothesis. New instances can be classified by combining the predictions of multiple hypotheses, weighted by their probabilities.\n",
    "\n",
    "7. Define the concept of consistent learners ?\n",
    "Ans: Consistent Learners: A learner L using a hypothesis H and training data D is said to be a consistent learner if it always outputs a hypothesis with zero error on D whenever H contains such a hypothesis. • By definition, a consistent learner must produce a hypothesis in the version space for H given D.\n",
    "\n",
    "8. Write any two strengths of Bayes classifier ?\n",
    "Ans: This algorithm works quickly and can save a lot of time. Naive Bayes is suitable for solving multi-class prediction problems. If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data.\n",
    "\n",
    "9. Write any two weaknesses of Bayes classifier ?\n",
    "Ans: The greatest weakness of the naïve Bayes classifier is that it relies on an often-faulty assumption of equally important and independent features which results in biased posterior probabilities.\n",
    "\n",
    "10. Explain how Naïve Bayes classifier is used for:\n",
    "Text classification\n",
    "Spam filtering\n",
    "Market sentiment analysis\n",
    "Ans: Navie Bayes Classifier is used for:\n",
    "\n",
    "Text classification: The Naive Bayes classifier is a simple classifier that classifies based on probabilities of events. It is the applied commonly to text classification. With the training set, we can train a Naive Bayes classifier which we can use to automaticall categorize a new sentence.\n",
    "Spam filtering:\n",
    "Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam. It is one of the oldest ways of doing spam filtering, with roots in the 1990s.\n",
    "Market sentiment analysis:\n",
    "Market Sentiment analysis is a field dedicated to extracting subjective emotions and feelings from text. One common use of sentiment analysis is to figure out if a text expresses negative or positive feelings. Naive Bayes is a popular algorithm for classifying text.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
